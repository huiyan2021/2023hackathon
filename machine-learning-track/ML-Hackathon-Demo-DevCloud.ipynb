{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b7ff3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "# <b><div style=\"color:#211894;font-size:100%;text-align:center\">欢迎来到黑客马拉松机器学习分赛道！ | Welcome to the Machine Learning Track of the Hackathon! 🚀</div></b>\n",
    "\n",
    "## <div style=\"text-align:center;color:#211894;font-size:90%\">Created By: Kelli Belcher</div>\n",
    "\n",
    "## <a id=\"TOC\">目录 | Table of Contents</a>\n",
    "- [1. 简介 | Introduction](#1)\n",
    "    - [1.1 数据描述 | Data Description](#1_1)\n",
    "    - [1.2 硬件 | Hardware](#1_2) \n",
    "- [2. 探索性数据分析(EDA) | Exploratory Data Analysis (EDA)](#2)  \n",
    "    - [2.1 数值变量 EDA | EDA of Numerical Variables](#2_1)\n",
    "        - [2.1.1 一元分布 | Univariate Distributions](#2_1_1)  \n",
    "        - [2.1.2 二元分布 | Bivariate Distributions](#2_1_2)\n",
    "    - [2.2 类别变量 EDA | EDA of Categorical Variables](#2_2)\n",
    "    - [2.3 相关性 | Correlations](#2_3)\n",
    "- [3. 建模 | Modeling](#3)\n",
    "    - [3.1 英特尔® Extension for Scikit-learn | Intel&reg; Extension for Scikit-learn](#3_1)\n",
    "    - [3.2 XGBoost 与英特尔® Daal4py | XGBoost with Intel&reg; Daal4py](#3_2)\n",
    "- [4. 结论 | Summary](#4)\n",
    "- [5. 参考资料 | References](#5)\n",
    "\n",
    "# <a class=\"anchor\" id=\"1\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #dc98ff, #251cab)\">1 | 简介 | Introduction</div></a>\n",
    "\n",
    "欢迎来到黑客马拉松机器学习 (ML) 分赛道！该参考 notebook 将演示如何使用<b>[英特尔® AI 分析工具套件（AI 套件）](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html#gs.b572eh)</b>中的多个加速 python 库，优化 ML 工作流的训练周期、预测吞吐量和准确率。本 notebook 中使用的主要的库包括：\n",
    "\n",
    "Welcome to the Machine Learning (ML) Track of the Hackathon! This reference notebook will demonstrate how to optimize the training cycles, prediction throughput, and accuracy of your ML workflow using several accelerated python libraries within the <b>[Intel<sup>&reg;</sup> AI Analytics Toolkit (AI Kit)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html#gs.b572eh)</b>. The main libraries we'll be working with in this notebook are:\n",
    "- <b>[英特尔® Modin* 分发版 | Intel<sup>&reg;</sup> Distribution of Modin*](https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-of-modin.html#gs.9hqdj4)</b>\n",
    "- <b>[英特尔® Extension for Scikit-learn* | Intel<sup>&reg;</sup> Extension for Scikit-learn*](https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html#gs.8txte9)</b>\n",
    "- <b>[针对英特尔® 架构优化的 XGBoost | XGBoost Optimized for Intel<sup>&reg;</sup> Architecture](https://www.intel.com/content/www/us/en/developer/articles/technical/xgboost-optimized-architecture-getting-started.html)</b>\n",
    "- <b>[英特尔® Daal4py | Intel<sup>&reg;</sup> Daal4py](https://intelpython.github.io/daal4py/)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2696fb-adde-4f45-9d42-1995fa597124",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a class=\"anchor\" id=\"1_1\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">1.1 | 数据集介绍 | Data Description</span></a>\n",
    "\n",
    "本教程中的数据集使用英特尔构建的开源<b>[预测性资产维护 AI 参考套件](https://github.com/oneapi-src/predictive-health-analytics)</b>生成，包括 100,000 个不同的电线杆，以及反映电线杆整体健康状态的 30 多个特征。我们的目标变量 <b>Asset_Label</b> 是一个二元指标，表示电线杆是否需要维护。人工识别问题的准确率不到 50%，并且维护和更换电线杆的成本超过 100 亿美元。正确预测更换电线杆的概率将帮助公司主动维护资产，避免断电和停工，节省运营成本。请点击此链接，获得该开源参考套件的代码，并了解更多信息：https://github.com/oneapi-src/predictive-health-analytics.\n",
    "\n",
    "为节省时间，我们把生成好的数据保存为`media/data_100000.pkl`.\n",
    "\n",
    "The dataset in this tutorial was generated using the open source <b>[Predictive Asset Maintenance AI Reference Kit](https://github.com/oneapi-src/predictive-health-analytics)</b> built by Intel&reg; and consists of 100,000 different utility poles with over 30 features on the overall health of the utility. Our target variable, <b>`Asset_Label`</b> is a binary indicator, representing whether or not the utility whether or not the utility pole requires maintenance. Manual problem identifications are less than 50% accurate and the costs of maintenance and replacement of utility poles are over \\\\$10 billion$^1$. Correctly predicting the probability of a pole replacement will help the company proactively maintain assets and avoid outages, downtime, and operational costs. You can get the code for this open-source reference kit and find out more about it by clicking on this link: https://github.com/oneapi-src/predictive-health-analytics.\n",
    "\n",
    "To save time, we prepare the dataset and save as `media/data_100000.pkl`.\n",
    "\n",
    "## <a class=\"anchor\" id=\"1_2\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">1.2 | 硬件 | Hardware</span></a>\n",
    "\n",
    "\n",
    "在黑客马拉松的本次活动中，我们推荐使用第三代英特尔® 至强® 系列处理器或第四代至强® 系列处理器，英特尔® AVX-512 指令配合相应的软件及组件能实现更好的性能，第三代至强® 系列处理器 Ice Lake CPU 架构，是很多主流云服务提供商都在使用的硬件。它提供更出色的性能，成本和复杂性均低于使用 GPU 平台。此外，相比默认的 Scikit-Learn，英特尔优化版本的速度提升了高达 10 到 100 倍（SVC 和 kNN 预测）$^2$。参赛人员可以利用英特尔免费为您提供的 [oneAPI DevCloud云测试环境](https://devcloud.intel.com/oneapi/) 中的各种硬件及计算能力，或利用自己的笔记本或其他适合的硬件环境作为比赛代码的开发、验证和最终代码递交的测试环境。 \n",
    "\n",
    "In this track of the Hackathon, we recommend to use a 3rd Generation or 4th Generation Intel&reg; Xeon&reg; based processor, software compoents will benefit from hardware features such as Intel® AVX-512 instructions by deliver better preformance. 3rd Generation ntel Intel&reg; Xeon&reg; based processor - which is an Ice Lake CPU, is a popular hardware platform adopted by mainstream cloud servcie providers. It deliver competitive performance without the likely added cost and complexity of switching to a GPU platform and offer up to 10-100x faster Intel-optimized versions over default Scikit-Learn (SVC & kNN predict)$^2$. The participants can also utilize the free [oneAPI Developer Cloud](https://devcloud.intel.com/oneapi/) test environment and its hardware and computing capabilities, your own notebook or other appropriate development platform for code development, validataion and as your final test platform for code submission.\n",
    "\n",
    "通过下面的命令，可以查看硬件环境中对应CPU的相应硬件能力 (仅作为示例)\n",
    "\n",
    "Through the below command, user will be able to view the hardware capability of the respective CPU （for illustration purpose only）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ef17f-397e-4a0f-bc7b-9a851674e5b6",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"1_3\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">1.3 | 导入库和数据集 | Import Libraries and Dataset</span></a>\n",
    "\n",
    "首先，使用<b>[英特尔® Modin 分发版](https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-of-modin.html#gs.9hqdj4)</b>处理与探索数据。英特尔® Modin 分发版是一个分布式 DataFrame 库，可根据数据集的大小无缝扩展 pandas 工作流，支持从 1 MB 到 1 TB 以上的数据集。采用标准 pandas 一次只能使用一个核心。然而，借助 Modin Dask* 引擎，您可以使用所有可用核心，从而更快速地处理较大的数据集。您可以运行以下代码，利用采用 Dask 引擎的 Modin：\n",
    "\n",
    "To get started, we'll be using the <b>[Intel&reg; Distribution of Modin](https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-of-modin.html#gs.9hqdj4)</b> to process and explore the data. The Intel&reg; Distribution of Modin is a distributed DataFrame library designed to seamlessly scale your pandas workflow with the size of your dataset, supporting datasets that range from 1 MB to 1 TB+. With pandas, only one core is used at a time. However, with Modin's Dask* engine, all of the available cores are used, which allows you to work with very large datasets at much faster speeds. To utilize Modin with the Dask engine, you can use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f915b5-3cef-4c68-9b8b-3bc896295e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init(_memory=16000 * 1024 * 1024, object_store_memory=500 * 1024 * 1024,_driver_object_store_memory=500 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d037511-2a01-491f-9a69-b9fd6a109258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as pd\n",
    "from modin.config import Engine\n",
    "Engine.put(\"ray\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef60409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import daal4py as d4p\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "import warnings\n",
    "import pandas\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pio.renderers.default='png' \n",
    "intel_pal, color=['#0071C5','#FCBB13'], ['#7AB5E1','#FCE7B2']\n",
    "temp=dict(layout=go.Layout(font=dict(family=\"Franklin Gothic\", size=12), \n",
    "                           height=500, width=1000))\n",
    "\n",
    "# Read data\n",
    "data = pandas.read_pickle('media/data_100000.pkl')\n",
    "print(\"Data shape: {}\\n\".format(data.shape))\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88aa08c",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"2\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #251cab, #3eb4f4, #251cab)\">2 | 探索性数据分析 | Exploratory Data Analysis</div></a>\n",
    "[返回至目录 | Back to Table of contents](#TOC)\n",
    "\n",
    "在工作流的下一步，我们将讨论： | In the next step of the workflow, we'll explore:\n",
    "- 查看数据集中的<b><span style=\"color:#211894;text-align:left\">缺失值</span></b> 和<b><span style=\"color:#211894;text-align:left\">重复值</span></b>。\n",
    "- 数据集的描述性统计，包括<b><span style=\"color:#211894;text-align:left\">均值</span></b>、<b><span style=\"color:#211894;text-align:left\">最小值</span></b>、<b><span style=\"color:#211894;text-align:left\">最大值</span></b>、<b><span style=\"color:#211894;text-align:left\">标准差</span></b>、<b><span style=\"color:#211894;text-align:left\">偏度</span></b>和<b><span style=\"color:#211894;text-align:left\">峰度</span></b>。\n",
    "- 目标变量 <b>`Asset_Label`</b> 的分布。\n",
    "- 数值和类别特征的<b><span style=\"color:#211894;text-align:left\">一元分布</span></b>和<b><span style=\"color:#211894;text-align:left\">二元分布</span></b>。\n",
    "- 数据集中变量之间的<b><span style=\"color:#211894;text-align:left\">相关性</span></b>。\n",
    "<br>\n",
    "\n",
    "\n",
    "- Checking for <b><span style=\"color:#211894;text-align:left\">missing values</span></b> and <b><span style=\"color:#211894;text-align:left\">duplicates</span></b> in the dataset.\n",
    "- Descriptive statistics of the dataset, including the <b><span style=\"color:#211894;text-align:left\">mean</span></b>, <b><span style=\"color:#211894;text-align:left\">min</span></b>, <b><span style=\"color:#211894;text-align:left\">max</span></b>, <b><span style=\"color:#211894;text-align:left\">standard deviation</span></b>, <b><span style=\"color:#211894;text-align:left\">skewness</span></b>, and <b><span style=\"color:#211894;text-align:left\">kurtosis</span></b>.\n",
    "- The distribution of the target variable, <b>`Asset_Label`</b>.\n",
    "- <b><span style=\"color:#211894;text-align:left\">Univariate</span></b> and <b><span style=\"color:#211894;text-align:left\">bivariate</span></b> distributions of the numerical and categorical features.\n",
    "- <b><span style=\"color:#211894;text-align:left\">Correlations</span></b> between the variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c688f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.isna().sum())\n",
    "missing=data.isna().sum().sum()\n",
    "duplicates=data.duplicated().sum()\n",
    "print(\"\\nThere are {:,.0f} missing values in the data.\".format(missing))\n",
    "print(\"There are {:,.0f} duplicate records in the data.\".format(duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40423271",
   "metadata": {},
   "source": [
    "数据集看上去非常干净，没有缺失或重复记录。那么，让我们看一下数据集的描述性统计。描述性统计提供了数据分布的集中趋势、离散和形态，不包括 `NaN` 值。 | At first glance, the dataset appears to be fairly clean with no missing or duplicate records. Let's now look at the descriptive statistics of the dataset. Descriptive statistics provide a numerical summary of the central tendency, dispersion, and shape of the data's distribution, excluding `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6100c2-cd01-4e23-b559-61cebca517cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to display descriptive statistics of numerical variables,\n",
    "    includes skewness & kurtosis.   \n",
    "    \"\"\"\n",
    "    \n",
    "    df=data.describe()\n",
    "    skewness=data.skew()\n",
    "    kurtosis=data.kurtosis()\n",
    "    df=df.append([skewness, kurtosis],ignore_index=True)\n",
    "    idx=pd.Series(['count','mean','std','min','25%','50%','75%','max','skewness','kurtosis'],name='Summary Statistic')\n",
    "    df=pd.concat([df,idx], axis=1).set_index('Summary Statistic')\n",
    "    display(df.style.format('{:,.3f}').\n",
    "        background_gradient(subset=(df.index[1:],df.columns[:]),\n",
    "                            cmap='GnBu'))\n",
    "\n",
    "display_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3defd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_target_dist(target_col):\n",
    "    \n",
    "    \"\"\"Function to display distribution of the target variable\"\"\"\n",
    "    \n",
    "    target=data[target_col].value_counts(normalize=True)\n",
    "    target.rename(index={1:'State 1',0:'State 0'},inplace=True)\n",
    "    fig=go.Figure()\n",
    "    fig.add_trace(go.Pie(labels=target.index, values=target*100, hole=.45, \n",
    "                         text=target.index, sort=False, showlegend=False,\n",
    "                         marker=dict(colors=color,line=dict(color=intel_pal,width=2.5)),\n",
    "                         hovertemplate = \"%{label}: <b>%{value:.2f}%</b><extra></extra>\"))\n",
    "    fig.update_layout(template=temp, title='Target Distribution',width=700,height=450,\n",
    "                      uniformtext_minsize=15, uniformtext_mode='hide')\n",
    "    fig.show() \n",
    "    \n",
    "plot_target_dist(target_col='Asset_Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030c629-dc42-4f52-b62a-057cdcbb7865",
   "metadata": {},
   "source": [
    "如图中的黄色部分所示，在我们的数据中，约 40% 的电线杆被认定为需要维修。考虑到目标变量的分布有些失衡，我们将在交叉验证中使用<b><span style=\"color:#211894;text-align:left\">分层抽样</span></b>。 | About 40% of poles in our data have been identified as requiring maintenance, shown in the yellow portion of the chart. Given the imbalance in the distribution of our target variable, we will use <b><span style=\"color:#211894;text-align:left\">stratified sampling</span></b> during cross-validation.\n",
    "\n",
    "## <a class=\"anchor\" id=\"2_1\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #009ff2, #251cab)\">2.1 | 数值变量 EDA | EDA of Numerical Variables</div></a>\n",
    "\n",
    "[返回至目录 | Back to Table of Contents](#TOC)\n",
    "\n",
    "<br>\n",
    "\n",
    "<b><a class=\"anchor\" id=\"2_1_1\"><span style=\"padding:0px;color:#211894;margin:0;font-size:120%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">2.1.1 | 一元分布 | Univariate Distributions</span></a></b>\n",
    "\n",
    "在对目标分布有了更好的了解后，让我们深入解读数据集中的特征，以识别特定模式。下图是数值变量的核密度估计 (KDE) 图，使用概率密度函数直观显示数据形态。\n",
    "\n",
    "Now that we have a better understanding of the target distribution, let's take a closer look at the features in the dataset to identify any patterns. The graphs below show the Kernel Density Estimation (KDE) plots of the numerical variables, which visually display the shape of the data using its probability density function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811931bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols,float_cols=[],['Asset_Label']\n",
    "for col in data.columns:\n",
    "    if data[col].value_counts().count()<10:\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        float_cols.append(col)\n",
    "        \n",
    "plot_df=data[float_cols]\n",
    "fig, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "fig.suptitle('Distribution of Numerical Variables',fontsize=16)\n",
    "row=0\n",
    "col=[0,1]*2\n",
    "for i, column in enumerate(plot_df.columns[1:]):\n",
    "    if (i!=0)&(i%2==0):\n",
    "        row+=1\n",
    "    sns.kdeplot(x=column, hue='Asset_Label', palette=intel_pal[::-1], hue_order=[1,0], \n",
    "                label=['State 1','State 0'], data=plot_df, \n",
    "                fill=True, linewidth=2.5, legend=False, ax=ax[row,col[i]])\n",
    "    ax[row,col[i]].tick_params(left=False, bottom=False)\n",
    "    ax[row,col[i]].set(title='\\n\\n{}'.format(column), xlabel='', ylabel=('Density' if i%2==0 else ''))\n",
    "\n",
    "handles, _ = ax[0,0].get_legend_handles_labels() \n",
    "fig.legend(labels=['State 1','State 0'], handles=reversed(handles), ncol=2, bbox_to_anchor=(0.18, 0.99))\n",
    "sns.despine(bottom=True, trim=True)\n",
    "plt.tight_layout(rect=[0, 0.2, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa0855-e6eb-4676-9a3d-64332513502a",
   "metadata": {},
   "source": [
    "从上图可以看出，变量 `Pole_Height` 具有钟形曲线，呈正态分布，而其他变量更接近均匀的非正态分布。 | In the graphs above, we see the variable `Pole_Height` has a bell-shaped curve following a normal distribution, while the remaining variables more closely resemble uniform, non-normal distributions.\n",
    "\n",
    "<b><a class=\"anchor\" id=\"2_1_2\"><span style=\"padding:0px;color:#211894;margin:0;font-size:120%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">2.1.2 | 二元分布 | Bivariate Distributions</span></a></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cee6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_subplots(rows=2,cols=2, subplot_titles=float_cols[1:])\n",
    "col=[1,2]*2\n",
    "row=0\n",
    "pal=sns.color_palette(\"GnBu\",20).as_hex()[9:][::3]\n",
    "for i, column in enumerate(data[float_cols].columns[1:]):\n",
    "    if i%2==0:\n",
    "        row+=1\n",
    "    df = pd.concat([data[column],data['Asset_Label']],axis=1)\n",
    "    df['bins'] = pd.cut(df[column],300)\n",
    "    df['mean'] = df.bins.apply(lambda x: x.mid)\n",
    "    df = df.groupby('mean')[column,'Asset_Label'].transform('mean')\n",
    "    df = df.drop_duplicates(subset=[column]).sort_values(by=column)\n",
    "    fig.add_trace(go.Scatter(x=df[column], y=df.Asset_Label, name=column,\n",
    "                             marker_color=pal[i],showlegend=False),\n",
    "                  row=row, col=col[i])\n",
    "    fig.update_xaxes(zeroline=False, row=row, col=col[i])\n",
    "    if i%2==0:\n",
    "        fig.update_yaxes(title='Target Probabilitiy',row=row,col=col[i])\n",
    "fig.update_layout(template=temp, title='Feature Relationships with Target', \n",
    "                  hovermode=\"x unified\",height=700,width=900)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced1771-2ef3-4398-8e7e-f9e2b7465ca2",
   "metadata": {},
   "source": [
    "为了识别目标变量和连续特征之间的关系，上图描述了每个特征中所有值的目标概率。在新旧程度 `Age` 中，我们看到使用超过 45 年和不到 5 年的电线杆维修概率更高，而在海拔高度  `Elevation` 中，海拔低于 1,000 英尺的维修概率更高。 此外，我们发现目标和特征之间存在非线性关系，这表明我们可能要尝试非参数模型。下方的散点图矩阵进一步印证了这一点，我们发现数据的数值特征之间线性度极弱，相关性较低。 \n",
    "<br>\n",
    "\n",
    "To identify the relationships between the target variable and the continuous features, the graphs above depict the target probability across the values in each feature. In `Age`, we see the probability tends to be higher in poles that are above about 45 years and less than 5 years, while in `Elevation` this is true for poles that are below about 1,000 feet. In addition, we see there are non-linear relationships between the target and the features, which suggests we may want to try a nonparametric model. This is further demonstrated in the scatterplot matrix below, where we see very weak linearity and low correlations between the numerical features in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df=data[float_cols]\n",
    "fig, ax = plt.subplots(4,4, figsize=(16,18))\n",
    "fig.suptitle('Scatterplot Matrix of Numeric Variables\\nLog-Transformed',fontsize=16)\n",
    "for i, col in enumerate(float_cols[1:]):\n",
    "    for j, iter_col in enumerate(float_cols[1:]):\n",
    "        ax[i,j].hexbin(x=iter_col, y=col, data=plot_df, bins='log', gridsize=40, cmap='coolwarm')\n",
    "        ax[i,j].set(xlabel=iter_col, ylabel=(col if j%4==0 else ''))\n",
    "        ax[i,j].text(plot_df[iter_col].median(), plot_df[col].max(), \n",
    "                     'Correlation: {:.4f}'.format(plot_df[[col,iter_col]].corr().iloc[1,0]), \n",
    "                   ha=\"center\", va=\"center\",bbox=dict(boxstyle=\"round,pad=0.3\",fc=\"white\"))\n",
    "        ax[i,j].tick_params(left=False,bottom=False)    \n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f8ba2",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"2_2\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #009ff2, #251cab)\">2.2 | 类别变量 EDA | EDA of Categorical Variables</div></a>\n",
    "\n",
    "[返回至目录 | Back to Table of Contents](#TOC)\n",
    "\n",
    "我们在前面探讨了数值变量，接下来看一下类别特征的分布。我们已使用独热编码对数据集中的类别特征进行了预处理，独热编码定性地表示变量存在与否，1 代表存在，0 代表不存在。下图显示了每个特征发生的频率，按照目标标签进行着色。 | Now that we have explored our numerical variables, let's take a look at the distributions in the categorical features. The categorical features in the dataset have already been preprocessed using  one-hot encoding, which qualitatively represents the presence or absence of the variable with a corresponding 1 or 0, respectively. The graphs below show the frequency of the occurrence of each feature, colored by the target label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a50cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=10, cols=3, subplot_titles=[c for c in cat_cols if c!='Asset_Label'])\n",
    "rgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.6)) for i in intel_pal]\n",
    "row=0\n",
    "c=[1,2,3]*10\n",
    "for i, col in enumerate(c for c in cat_cols if c!='Asset_Label'):\n",
    "    if i%3==0:\n",
    "        row+=1\n",
    "    df=data.groupby(col)['Asset_Label'].value_counts().rename('count').reset_index()\n",
    "    fig.add_trace(go.Bar(x=df[df.Asset_Label==1][col], y=df[df.Asset_Label==1]['count'],width=.35,\n",
    "                         marker_color=rgb[1], marker_line=dict(color=intel_pal[1],width=2.5), \n",
    "                         hovertemplate='Value: %{x}<br>Count: %{y}',\n",
    "                         name='State 1', showlegend=(True if i==0 else False)),\n",
    "                  row=row, col=c[i])\n",
    "    fig.add_trace(go.Bar(x=df[df.Asset_Label==0][col], y=df[df.Asset_Label==0]['count'],width=.35,\n",
    "                         marker_color=rgb[0], marker_line=dict(color=intel_pal[0],width=2.5),\n",
    "                         hovertemplate='Value: %{x}<br>Count: %{y}',\n",
    "                         name='State 0', showlegend=(True if i==0 else False)),\n",
    "                  row=row, col=c[i])\n",
    "    if i%3==0:\n",
    "        fig.update_yaxes(title='Frequency',row=row,col=c[i])\n",
    "fig.update_layout(template=temp,title=\"Distributions of Categorical Variables\",\n",
    "                  legend=dict(orientation=\"h\",yanchor=\"bottom\",y=1.025,xanchor=\"right\",x=.2),\n",
    "                  barmode='group',height=2000,width=900)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b7d6c-0b72-4fa8-9511-c53b6472b19c",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"2_3\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #009ff2, #251cab)\">2.3 | 相关性 | Correlations</div></a>\n",
    "\n",
    "[返回至目录 | Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(corr):\n",
    "    \"\"\"\n",
    "    Function to plot bottom left triangle of correlation matrix\n",
    "    \"\"\"\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool))[1:,:-1]\n",
    "    corr=corr.iloc[1:,:-1].copy()\n",
    "    fig, ax = plt.subplots(figsize=(26,22))   \n",
    "    sns.heatmap(corr, mask=mask, vmin=-1, vmax=1, center=0, annot=True, fmt='.2f', \n",
    "                cmap='YlGnBu_r',lw=2, annot_kws={'fontsize':10,'fontweight':'bold'}, cbar=True)\n",
    "    ax.tick_params(left=False,bottom=False)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right',fontsize=12)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(),fontsize=12)\n",
    "    plt.title('Correlations between Utility Asset Maintenance Data\\n', fontsize=24)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_target_corr(corr, target_col): \n",
    "    \"\"\"\n",
    "    Function to plot a bar chart of correlations between target and features, sorted in descending order\n",
    "    \"\"\"\n",
    "    corr=corr[target_col].sort_values(ascending=False)[1:]\n",
    "    pal=sns.color_palette(\"RdYlBu\",37).as_hex()\n",
    "    pal=[j for i,j in enumerate(pal) if i not in (17,18)]\n",
    "    rgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.8)) for i in pal] \n",
    "    \n",
    "    fig=go.Figure()\n",
    "    fig.add_trace(go.Bar(x=corr.index, y=corr, marker_color=rgb,\n",
    "                         marker_line=dict(color=pal,width=2),\n",
    "                         hovertemplate='%{x} correlation with Target = %{y}',\n",
    "                         showlegend=False, name=''))\n",
    "    fig.update_layout(template=temp, title='Feature Correlations with Target (Asset Label)', \n",
    "                      yaxis_title='Correlation', margin=dict(b=160), xaxis_tickangle=45)\n",
    "    fig.show()\n",
    "    \n",
    "corr=data.corr()\n",
    "plot_corr(corr=corr)\n",
    "plot_target_corr(corr=corr, target_col='Asset_Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e4070",
   "metadata": {},
   "source": [
    "从上方的散点图矩阵可以看出，数值特征之间的相关性很低，数据集的其他特征也是如此。在我们的目标变量 Asset_Label 中，Original_Treatment_Untreated有最强的正相关，相关性为 0.311，District_W有最强的负相关，相关性为 -0.172\n",
    "<br>\n",
    "As we saw in the scatterplot matrix above, there were very low correlations among the numerical features, which also remains true between the rest of the features in the dataset. Among our target variable, `Asset_Label`, the highest positive association exists with `Original_Treatment_Untreated` at 0.311 and the strongest negative relationship between `District_W` at -0.172\n",
    "\n",
    "# <a class=\"anchor\" id=\"3\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #3eb4f4, #251cab)\">3 | 建模 | Modeling</div></a>\n",
    "\n",
    "[返回至目录 | Back to Table of Contents](#TOC)\n",
    "\n",
    "在本节，我们将探索如何使用不同的机器学习模型预测电线杆是否需要维修。二元分类任务可选择不同的模型。最常用的模型包括逻辑回归、朴素贝叶斯、K 最近邻、支持向量机以及集成方法，例如随机森林和 XGBoost。如上图所示，由于特征和目标变量之间有非线性关系，因此我们将比较两种非参数模型的优劣：支持向量机和 XGBoost。\n",
    "<br>\n",
    "\n",
    "In this section, we will explore different Machine Learning models to predict whether or not a pole will need maintenance. With binary classification tasks, there are many different models to choose from. Some of the most common models include Logistic Regression, Naive Bayes, K-Nearest Neighbors, Support Vector Machines, and ensemble methods, like Random Forests and XGBoost. Since we saw there were nonlinear relationships between the features and the target variable in the graphs above, we will compare two nonparametric models: Support Vector Machines and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(data, target_col, test_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to scale and split the data into training and test sets\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = RobustScaler()   \n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=21)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Train Shape: {}\".format(X_train_scaled.shape))\n",
    "    print(\"Test Shape: {}\".format(X_test_scaled.shape))\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def plot_model_res(model_name, y_test, y_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to plot ROC/PR Curves and predicted target distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    intel_pal=['#0071C5','#FCBB13']\n",
    "    color=['#7AB5E1','#FCE7B2']\n",
    "    \n",
    "    ## ROC & PR Curve ##\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    auprc = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, \n",
    "                        shared_yaxes=True, \n",
    "                        subplot_titles=['Receiver Operating Characteristic<br>(ROC) Curve',\n",
    "                                        'Precision-Recall Curve<br>AUPRC = {:.3f}'.format(auprc)])\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=np.linspace(0,1,11), y=np.linspace(0,1,11), \n",
    "                             name='Baseline',mode='lines',legendgroup=1,\n",
    "                             line=dict(color=\"Black\", width=1, dash=\"dot\")), row=1,col=1)    \n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, line=dict(color=intel_pal[0], width=3), \n",
    "                             hovertemplate = 'True positive rate = %{y:.3f}, False positive rate = %{x:.3f}',\n",
    "                             name='AUC = {:.4f}'.format(roc_auc),legendgroup=1), row=1,col=1)\n",
    "    fig.add_trace(go.Scatter(x=recall, y=precision, line=dict(color=intel_pal[0], width=3), \n",
    "                             hovertemplate = 'Precision = %{y:.3f}, Recall = %{x:.3f}',\n",
    "                             name='AUPRC = {:.4f}'.format(auprc),showlegend=False), row=1,col=2)\n",
    "    fig.update_layout(template=temp, title=\"{} ROC and Precision-Recall Curves\".format(model_name), \n",
    "                      hovermode=\"x unified\", width=900,height=500,\n",
    "                      xaxis1_title='False Positive Rate (1 - Specificity)',\n",
    "                      yaxis1_title='True Positive Rate (Sensitivity)',\n",
    "                      xaxis2_title='Recall (Sensitivity)',yaxis2_title='Precision (PPV)',\n",
    "                      legend=dict(orientation='v', y=.07, x=.45, xanchor=\"right\",\n",
    "                                  bordercolor=\"black\", borderwidth=.5))\n",
    "    fig.show()\n",
    "    \n",
    "    ## Target Distribution ##     \n",
    "    plot_df=pd.DataFrame.from_dict({'State 0':(len(y_prob[y_prob<=0.5])/len(y_prob))*100, \n",
    "                                    'State 1':(len(y_prob[y_prob>0.5])/len(y_prob))*100}, \n",
    "                                   orient='index', columns=['pct'])\n",
    "    fig=go.Figure()\n",
    "    fig.add_trace(go.Pie(labels=plot_df.index, values=plot_df.pct, hole=.45, \n",
    "                         text=plot_df.index, sort=False, showlegend=False,\n",
    "                         marker=dict(colors=color,line=dict(color=intel_pal,width=2.5)),\n",
    "                         hovertemplate = \"%{label}: <b>%{value:.2f}%</b><extra></extra>\"))\n",
    "    fig.update_layout(template=temp, title='Predicted Target Distribution',width=700,height=450,\n",
    "                      uniformtext_minsize=15, uniformtext_mode='hide')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715c96a",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"3_1\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">3.1 | 采用英特尔® Extension for Scikit-learn 的支持向量分类器 | Support Vector Classifier with Intel&reg; Extension for Scikit-learn</span></a>\n",
    "\n",
    "[返回至目录 | Back to Table of Contents](#TOC)\n",
    "\n",
    "我们演示的第一个模型是支持向量分类器 (SVC)。1992 年，Vapnik 和他的同事提出了支持向量分类器（作为算法），以最大化训练样本和决策边界之间的间隔 (margin)[[1](https://dl.acm.org/doi/10.1145/130385.130401)]。我们将从 Python Scikit-learn 软件包中导入 SVC 库，并利用[英特尔® Extension for Scikit-learn](https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html#gs.begztq)提供的加速。\n",
    "<br>\n",
    "\n",
    "The first model we will demonstrate is a Support Vector Classifier (SVC). Support Vector Classifiers were introduced in 1992 by Vapnik and colleagues as an algorithm that maximizes the margin between the training patterns and the decision boundary [[1](https://dl.acm.org/doi/10.1145/130385.130401)]. We will be importing the SVC library from the Scikit-learn package in Python and utilizing the accelerations provided with the [Intel&reg; Extension for Scikit-learn](https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html#gs.begztq). \n",
    "\n",
    "英特尔® Extension for Scikit-learn 可跨单节点和多节点配置，无缝集成面向英特尔® CPU 和 GPU 的 scikit-learn 应用，同时缩短算法运行时间。如下方的代码单元所示，只需调用 patch_sklearn() 函数，即可利用加速。您可以继续使用相同的 AI 软件包和 scikit-learn 库，不用再修改代码。您可以利用补丁，将支持的常见 scikit-learn 算法替换为它们的优化版本。请访问我们的[开发人员指南](https://intel.github.io/scikit-learn-intelex/index.html)，进一步了解支持的算法。\n",
    "<br>\n",
    "\n",
    "The Intel&reg; Extension for Scikit-learn offers a seamless integration with scikit-learn applications for Intel&reg; CPUs and GPUs across single- and multi-node configurations while reducing algorithm run time. To take advantage of the accelerations, all you need to do is call the `patch_sklearn()` function as shown in the code cell below and continue using the same AI packages and scikit-learn libraries without any other changes to your code. The patch will replace supported stock scikit-learn algorithms with their optimized versions. To learn more about which algorithms are supported, please visit our [Developer Guide](https://intel.github.io/scikit-learn-intelex/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211463c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317ac2d",
   "metadata": {},
   "source": [
    "我们已经启用了英特尔® Extension for Scikit-learn，下一步是把数据分为训练和测试集，并使用分层 3 重交叉验证调优支持向量机的超参数。超参数调优的结果和测试集上的模型性能如下图所示。由于这是一项二元分类任务，因此，我们关注的主要评估指标是分数和 ROC 曲线下面积 (AUC)。\n",
    "<br>\n",
    "Now that we've enabled the Intel&reg; Extension for Scikit-learn, we will split our data into training and test sets and tune the hyperparameters of the Support Vector Machine using stratified 3-fold cross-validation. The results of the hyperparameter tuning and model performance on the test set are shown in the graphs below. As this is a binary classification task, the main evaluation metrics we will be focusing on are the $F_{1}$ score and the Area Under the ROC Curve (AUC). \n",
    "\n",
    "<b>[$F_{1}$ 数值](https://en.wikipedia.org/wiki/F-score)</b> 是阳性类精度和召回率之间的调和平均数，用 1 来表示。通过以下计算方法得出：\n",
    "\n",
    "\n",
    "The <b>[$F_{1}$ score](https://en.wikipedia.org/wiki/F-score)</b> represents the harmonic mean between the Precision and Recall of the positive class, indicated by a 1. It can be derived using the following calculation: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$F_1 = \\frac{2 * Precision * Recall}{Precision + Recall} = \\frac{2 * TP}{2 * TP + FP + FN}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "虽然大多数指标是使用 0.5 分类阈值计算的，但是 <b>[ROC 曲线下面积 (AUC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)</b>是所有可能阈值下模型性能的总体表示。您可以把它理解为从阳性类中随机选择的观测值比从阴性类中随机选择的观测值更高的概率。\n",
    "\n",
    "While most metrics are calculated using a classification threshold of 0.5, the <b>[Area Under the ROC Curve (AUC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)</b> is an aggregate representation of the model's performance across all possible thresholds. It can be interpreted as the probability that a randomly-selected observation from the positive class will be ranked more highly than a randomly-selected observation from the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ec64e-117d-4506-89b0-31aa2a18ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Train and Test datasets ##\n",
    "print(\"Preparing Train and Test datasets\")\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_data(data=data, \n",
    "                                                            target_col='Asset_Label', \n",
    "                                                            test_size=.25)\n",
    "## Initialize SVC model ##\n",
    "parameters = {\n",
    "    'class_weight': 'balanced',\n",
    "    'probability': True,\n",
    "    'random_state': 21}\n",
    "svc = SVC(**parameters)\n",
    "\n",
    "## Tune Hyperparameters ##\n",
    "strat_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=21)\n",
    "print(\"\\nTuning hyperparameters..\")\n",
    "grid = {\n",
    "    'C': np.logspace(-1, 1, 5),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }\n",
    "grid_search = RandomizedSearchCV(svc, param_distributions=grid, \n",
    "                                    cv=strat_kfold, n_iter=1, scoring='roc_auc', \n",
    "                                    verbose=1, n_jobs=-1, random_state=21)\n",
    "grid_search.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Done!\\nBest hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation AUC: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "svc = grid_search.best_estimator_\n",
    "svc_prob = svc.predict_proba(X_test)[:,1]\n",
    "svc_pred = pd.Series(svc.predict(X_test), name='Target')\n",
    "svc_auc = roc_auc_score(y_test, svc_prob)\n",
    "svc_f1 = f1_score(y_test, svc_pred)  \n",
    "   \n",
    "## Print model results ##\n",
    "print(\"\\nTest F1 accuracy: {:.2f}%, AUC: {:.5f}\".format(svc_f1*100,svc_auc))\n",
    "plot_model_res(model_name='SVC', y_test=y_test, y_prob=svc_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbf279",
   "metadata": {},
   "source": [
    "使用正则化参数约为 3.16 的多项式核，支持向量分类器在测试数据集上能实现的[$F_{1}$ 分数](https://en.wikipedia.org/wiki/F-score)为 86.12%，[ROC 曲线下面积 (AUC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)  为 0.91。\n",
    "在下一节，我们将尝试使用另一种机器学习算法 - XGBoost 模型来提升 SVC 的性能。\n",
    "\n",
    "Using a polynomial kernel with a regularization parameter of about 3.16, the Support Vector Classifier achieved an [$F_{1}$ score](https://en.wikipedia.org/wiki/F-score) of 86.12% and an [Area Under the ROC Curve (AUC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) of 0.91 on the test set.\n",
    "Next, we'll see if we we can improve on the performance of the SVC using a different type of machine learning algorithm, an XGBoost model.\n",
    "\n",
    "## <a class=\"anchor\" id=\"3_2\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">3.2 | 针对英特尔® 架构优化的 XGBoost | XGBoost Optimized for Intel&reg; Architectures</span></a>\n",
    "[返回至目录 | Back to Table of Contents](#TOC)\n",
    "\n",
    "在本节，我们将探讨极端梯度提升（eXtreme Gradient Boosting）算法，简称为 XGBoost。2016 年，Chen 和 Guestrin 将 XGBoost 作为一种可扩展端到端决策树提升系统发布出来。虽然它是近几年才出现的，但是已成为数据科学家广泛使用的一个模型，在多个机器学习挑战赛上大放异彩[[2](https://arxiv.org/abs/1603.02754)]。自 XGBoost 版本 0.81 以来，英特尔推出了多项优化并将其部署到上游的软件包，以最大限度地提升训练性能。\n",
    "\n",
    "In this section, we will explore the eXtreme Gradient Boosting algorithm, also known as XGBoost. XGBoost was introduced in 2016 by Chen and Guestrin as a scalable end-to-end decision tree boosting system. While it is still relatively new, it has become a widely used model by data scientists to achieve state-of-the-art results on many machine learning challenges [[2](https://arxiv.org/abs/1603.02754)]. Since XGBoost version 0.81, Intel&reg; has introduced many optimizations to maximize training performance that have been upstreamed into the package. \n",
    "\n",
    "步骤和上文的相同，首先将数据分为训练和测试集，然后使用分层 3 重交叉验证调优 XGBoost 模型的超参数。我们一旦找到了 XGBoost 模型的最佳超参数，便将其转换为 daal4py 模型，以便进一步改进预测时间性能。[Daal4py](https://intelpython.github.io/daal4py/) 可从 [英特尔® oneAPI 数据分析库 (oneDAL)](https://oneapi-src.github.io/oneDAL/)  中下载，它将利用底层的英特尔® 高级矢量扩展指令集（英特尔® AVX-512）硬件，最大限度地提高英特尔® 至强® 处理器上的梯度提升性能。\n",
    "\n",
    "We will follow the same steps as above, beginning with splitting our data into training and test sets and tuning the hyperparameters of the XGBoost model using stratified 3-fold cross-validation. Once we have found the best hyperparameters for the XGBoost model, we will convert it to a daal4py model for further improved performance on prediction time. [Daal4py](https://intelpython.github.io/daal4py/) can be downloaded from the [Intel&reg; oneAPI Data Analytics Library (oneDAL)](https://oneapi-src.github.io/oneDAL/) and utilizes the underlying Intel&reg; Advanced Vector Extensions (Intel&reg; AVX-512) hardware to maximize gradient boosting performance on Intel&reg; Xeon&reg; processors.\n",
    "\n",
    "使用 `get_gbt_model_from_xgboost()` 函数编写一行代码，即可将调优后的 XGBoost 模型轻松转换为 Daal4py。然后，您可以使用 Daal4py 的预测函数发送经过训练的模型和输入数据，以计算测试集上的概率。\n",
    "\n",
    "To convert a tuned XGBoost model to Daal4py is very easy to do in one line of code with the `get_gbt_model_from_xgboost()` function. Then, you can send the trained model along with the input data using Daal4py's prediction function to calculate the probabilities on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe326e9e-67f5-47b4-b955-41a62707c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Train and Test datasets ##\n",
    "print(\"Preparing Train and Test datasets\")\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_data(data=data,\n",
    "                                                            target_col='Asset_Label', \n",
    "                                                            test_size=.25)\n",
    "    \n",
    "## Initialize XGBoost model ##\n",
    "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "parameters = {'scale_pos_weight': ratio.round(2), \n",
    "                'tree_method': 'hist',\n",
    "                'random_state': 21}\n",
    "xgb_model = XGBClassifier(**parameters)\n",
    "\n",
    "## Tune hyperparameters ##\n",
    "strat_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=21)\n",
    "print(\"\\nTuning hyperparameters..\")\n",
    "grid = {'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        }\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid=grid, \n",
    "                            cv=strat_kfold, scoring='roc_auc', \n",
    "                            verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Done!\\nBest hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation AUC: {:.4f}\".format(grid_search.best_score_))\n",
    "    \n",
    "## Convert XGB model to daal4py ##\n",
    "xgb = grid_search.best_estimator_\n",
    "daal_model = d4p.get_gbt_model_from_xgboost(xgb.get_booster())\n",
    "\n",
    "## Calculate predictions ##\n",
    "daal_prob = d4p.gbt_classification_prediction(nClasses=2,\n",
    "    resultsToEvaluate=\"computeClassLabels|computeClassProbabilities\",\n",
    "    fptype='float').compute(X_test, daal_model).probabilities # or .predictions\n",
    "xgb_pred = pd.Series(np.where(daal_prob[:,1]>.5, 1, 0), name='Target')\n",
    "xgb_auc = roc_auc_score(y_test, daal_prob[:,1])\n",
    "xgb_f1 = f1_score(y_test, xgb_pred)  \n",
    "    \n",
    "## Plot model results ##\n",
    "print(\"\\nTest F1 Accuracy: {:.2f}%, AUC: {:.5f}\".format(xgb_f1*100, xgb_auc)) \n",
    "plot_model_res(model_name='XGBoost', y_test=y_test, y_prob=daal_prob[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc3bbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "XGBoost 模型将 F1 准确率和 AUC 分别提高至 90% 以上和 0.94。 | The XGBoost model improved both the F1 accuracy and the AUC to over 90% and 0.94, respectively.\n",
    "\n",
    "# <a class=\"anchor\" id=\"4\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #3eb4f4, #251cab)\">4 | 结论 | Conclusion</div></a>\n",
    "[返回至目录 | Back to Table of Contents](#TOC)\n",
    "\n",
    "谢谢您的阅读！本参考套件实现为公用事业客户提供了资产维护预测的性能优化指南，可轻松扩展至类似的行业和使用场景。如果您想了解更多信息并且下载英特尔® 预测性资产维护 AI 参考套件，请访问此链接：https://github.com/oneapi-src/predictive-health-analytics. 您也可以点击[此处](https://www.intel.com/content/www/us/en/developer/videos/optimize-utility-maintenance-prediction-ai-kit.html)，观看本 notebook 的视频演示。\n",
    "\n",
    "Thank you for reading! This reference kit implementation provides a performance-optimized guide around the prediction of Asset Maintenance for Utility customers that can easily be scaled across similar industries and use cases. If you would like to learn more and download the Intel&reg; Predictive Asset Maintenance AI Reference Kit, please visit this link: https://github.com/oneapi-src/predictive-health-analytics. You may also watch a video demo of this notebook [here](https://www.intel.com/content/www/us/en/developer/videos/optimize-utility-maintenance-prediction-ai-kit.html).\n",
    "\n",
    "## <a class=\"anchor\" id=\"5\"><span style=\"padding:0px;color:#251cab;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">参考资料 | References</span></a>\n",
    "\n",
    "1. Boser, B., Guyon, I., Vapnik, V. (1992, July). A training algorithm for optimal margin classifiers. In: <i>Proceedings of the Fifth Annual Workshop on Computational Learning Theory</i>.\n",
    "2. Chen, T., & Guestrin, C. (2016, August). Xgboost: A scalable tree boosting system. In: <i>Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining </i>(pp. 785-794).\n",
    "\n",
    "## <span style=\"padding:0px;color:#251cab;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">脚注 | Footnotes</span>\n",
    "$^1$ 电线杆：维护或更换。美国公用事业合作伙伴。2020 年 8 月 3 日。 | Utility Poles: Maintenance or Replacement. Utility Partners of America. August 3, 2020.  \n",
    "$^2$ 请参见 [www.intel.com/3gen-xeon-config](www.intel.com/3gen-xeon-config) 的第 [117] 条。结果可能不同。 | See [117] at [www.intel.com/3gen-xeon-config](www.intel.com/3gen-xeon-config). Results may vary.\n",
    "\n",
    "\n",
    "## <span style=\"padding:0px;color:#251cab;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">Notices & Disclaimers</span>\n",
    "Intel optimizations, for Intel compilers or other products, may not optimize to the same degree for non-Intel products.  \n",
    "Performance varies by use, configuration and other factors. Learn more on the Performance Index site.   \n",
    "Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure.   \n",
    "Your costs and results may vary.   \n",
    "Intel technologies may require enabled hardware, software or service activation.  \n",
    "&copy; Intel Corporation.  Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and brands may be claimed as the property of others.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack-ml-ray",
   "language": "python",
   "name": "hack-ml-ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "864135a569a0074a0e5fb4f63ce6676f9823c9f9a01192383dbd7b79504f9de8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
