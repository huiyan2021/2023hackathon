{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b7ff3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "# <b><div style=\"color:#211894;font-size:100%;text-align:center\">æ¬¢è¿æ¥åˆ°é»‘å®¢é©¬æ‹‰æ¾æœºå™¨å­¦ä¹ åˆ†èµ›é“ï¼ | Welcome to the Machine Learning Track of the Hackathon! ğŸš€</div></b>\n",
    "\n",
    "## <div style=\"text-align:center;color:#211894;font-size:90%\">Created By: Kelli Belcher</div>\n",
    "\n",
    "## <a id=\"TOC\">ç›®å½• | Table of Contents</a>\n",
    "- [1. ç®€ä»‹ | Introduction](#1)\n",
    "    - [1.1 æ•°æ®æè¿° | Data Description](#1_1)\n",
    "    - [1.2 ç¡¬ä»¶ | Hardware](#1_2) \n",
    "- [2. æ¢ç´¢æ€§æ•°æ®åˆ†æ(EDA) | Exploratory Data Analysis (EDA)](#2)  \n",
    "    - [2.1 æ•°å€¼å˜é‡ EDA | EDA of Numerical Variables](#2_1)\n",
    "        - [2.1.1 ä¸€å…ƒåˆ†å¸ƒ | Univariate Distributions](#2_1_1)  \n",
    "        - [2.1.2 äºŒå…ƒåˆ†å¸ƒ | Bivariate Distributions](#2_1_2)\n",
    "    - [2.2 ç±»åˆ«å˜é‡ EDA | EDA of Categorical Variables](#2_2)\n",
    "    - [2.3 ç›¸å…³æ€§ | Correlations](#2_3)\n",
    "- [3. å»ºæ¨¡ | Modeling](#3)\n",
    "    - [3.1 è‹±ç‰¹å°”Â® Extension for Scikit-learn | Intel&reg; Extension for Scikit-learn](#3_1)\n",
    "    - [3.2 XGBoost ä¸è‹±ç‰¹å°”Â® Daal4py | XGBoost with Intel&reg; Daal4py](#3_2)\n",
    "- [4. ç»“è®º | Summary](#4)\n",
    "- [5. å‚è€ƒèµ„æ–™ | References](#5)\n",
    "\n",
    "# <a class=\"anchor\" id=\"1\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #dc98ff, #251cab)\">1 | ç®€ä»‹ | Introduction</div></a>\n",
    "\n",
    "æ¬¢è¿æ¥åˆ°é»‘å®¢é©¬æ‹‰æ¾æœºå™¨å­¦ä¹  (ML) åˆ†èµ›é“ï¼è¯¥å‚è€ƒ notebook å°†æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨<b>[è‹±ç‰¹å°”Â® AI åˆ†æå·¥å…·å¥—ä»¶ï¼ˆAI å¥—ä»¶ï¼‰](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html#gs.b572eh)</b>ä¸­çš„å¤šä¸ªåŠ é€Ÿ python åº“ï¼Œä¼˜åŒ– ML å·¥ä½œæµçš„è®­ç»ƒå‘¨æœŸã€é¢„æµ‹ååé‡å’Œå‡†ç¡®ç‡ã€‚æœ¬ notebook ä¸­ä½¿ç”¨çš„ä¸»è¦çš„åº“åŒ…æ‹¬ï¼š\n",
    "\n",
    "Welcome to the Machine Learning (ML) Track of the Hackathon! This reference notebook will demonstrate how to optimize the training cycles, prediction throughput, and accuracy of your ML workflow using several accelerated python libraries within the <b>[Intel<sup>&reg;</sup> AI Analytics Toolkit (AI Kit)](https://www.intel.com/content/www/us/en/developer/tools/oneapi/ai-analytics-toolkit.html#gs.b572eh)</b>. The main libraries we'll be working with in this notebook are:\n",
    "- <b>[è‹±ç‰¹å°”Â® Modin* åˆ†å‘ç‰ˆ | Intel<sup>&reg;</sup> Distribution of Modin*](https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-of-modin.html#gs.9hqdj4)</b>\n",
    "- <b>[è‹±ç‰¹å°”Â® Extension for Scikit-learn* | Intel<sup>&reg;</sup> Extension for Scikit-learn*](https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html#gs.8txte9)</b>\n",
    "- <b>[é’ˆå¯¹è‹±ç‰¹å°”Â® æ¶æ„ä¼˜åŒ–çš„ XGBoost | XGBoost Optimized for Intel<sup>&reg;</sup> Architecture](https://www.intel.com/content/www/us/en/developer/articles/technical/xgboost-optimized-architecture-getting-started.html)</b>\n",
    "- <b>[è‹±ç‰¹å°”Â® Daal4py | Intel<sup>&reg;</sup> Daal4py](https://intelpython.github.io/daal4py/)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2696fb-adde-4f45-9d42-1995fa597124",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <a class=\"anchor\" id=\"1_1\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">1.1 | æ•°æ®é›†ä»‹ç» | Data Description</span></a>\n",
    "\n",
    "æœ¬æ•™ç¨‹ä¸­çš„æ•°æ®é›†ä½¿ç”¨è‹±ç‰¹å°”æ„å»ºçš„å¼€æº<b>[é¢„æµ‹æ€§èµ„äº§ç»´æŠ¤ AI å‚è€ƒå¥—ä»¶](https://github.com/oneapi-src/predictive-health-analytics)</b>ç”Ÿæˆï¼ŒåŒ…æ‹¬ 100,000 ä¸ªä¸åŒçš„ç”µçº¿æ†ï¼Œä»¥åŠåæ˜ ç”µçº¿æ†æ•´ä½“å¥åº·çŠ¶æ€çš„ 30 å¤šä¸ªç‰¹å¾ã€‚æˆ‘ä»¬çš„ç›®æ ‡å˜é‡ <b>Asset_Label</b> æ˜¯ä¸€ä¸ªäºŒå…ƒæŒ‡æ ‡ï¼Œè¡¨ç¤ºç”µçº¿æ†æ˜¯å¦éœ€è¦ç»´æŠ¤ã€‚äººå·¥è¯†åˆ«é—®é¢˜çš„å‡†ç¡®ç‡ä¸åˆ° 50%ï¼Œå¹¶ä¸”ç»´æŠ¤å’Œæ›´æ¢ç”µçº¿æ†çš„æˆæœ¬è¶…è¿‡ 100 äº¿ç¾å…ƒã€‚æ­£ç¡®é¢„æµ‹æ›´æ¢ç”µçº¿æ†çš„æ¦‚ç‡å°†å¸®åŠ©å…¬å¸ä¸»åŠ¨ç»´æŠ¤èµ„äº§ï¼Œé¿å…æ–­ç”µå’Œåœå·¥ï¼ŒèŠ‚çœè¿è¥æˆæœ¬ã€‚è¯·ç‚¹å‡»æ­¤é“¾æ¥ï¼Œè·å¾—è¯¥å¼€æºå‚è€ƒå¥—ä»¶çš„ä»£ç ï¼Œå¹¶äº†è§£æ›´å¤šä¿¡æ¯ï¼šhttps://github.com/oneapi-src/predictive-health-analytics.\n",
    "\n",
    "ä¸ºèŠ‚çœæ—¶é—´ï¼Œæˆ‘ä»¬æŠŠç”Ÿæˆå¥½çš„æ•°æ®ä¿å­˜ä¸º`media/data_100000.pkl`.\n",
    "\n",
    "The dataset in this tutorial was generated using the open source <b>[Predictive Asset Maintenance AI Reference Kit](https://github.com/oneapi-src/predictive-health-analytics)</b> built by Intel&reg; and consists of 100,000 different utility poles with over 30 features on the overall health of the utility. Our target variable, <b>`Asset_Label`</b> is a binary indicator, representing whether or not the utility whether or not the utility pole requires maintenance. Manual problem identifications are less than 50% accurate and the costs of maintenance and replacement of utility poles are over \\\\$10 billion$^1$. Correctly predicting the probability of a pole replacement will help the company proactively maintain assets and avoid outages, downtime, and operational costs. You can get the code for this open-source reference kit and find out more about it by clicking on this link: https://github.com/oneapi-src/predictive-health-analytics.\n",
    "\n",
    "To save time, we prepare the dataset and save as `media/data_100000.pkl`.\n",
    "\n",
    "## <a class=\"anchor\" id=\"1_2\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">1.2 | ç¡¬ä»¶ | Hardware</span></a>\n",
    "\n",
    "\n",
    "åœ¨é»‘å®¢é©¬æ‹‰æ¾çš„æœ¬æ¬¡æ´»åŠ¨ä¸­ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨ç¬¬ä¸‰ä»£è‹±ç‰¹å°”Â® è‡³å¼ºÂ® ç³»åˆ—å¤„ç†å™¨æˆ–ç¬¬å››ä»£è‡³å¼ºÂ® ç³»åˆ—å¤„ç†å™¨ï¼Œè‹±ç‰¹å°”Â® AVX-512 æŒ‡ä»¤é…åˆç›¸åº”çš„è½¯ä»¶åŠç»„ä»¶èƒ½å®ç°æ›´å¥½çš„æ€§èƒ½ï¼Œç¬¬ä¸‰ä»£è‡³å¼ºÂ® ç³»åˆ—å¤„ç†å™¨ Ice Lake CPU æ¶æ„ï¼Œæ˜¯å¾ˆå¤šä¸»æµäº‘æœåŠ¡æä¾›å•†éƒ½åœ¨ä½¿ç”¨çš„ç¡¬ä»¶ã€‚å®ƒæä¾›æ›´å‡ºè‰²çš„æ€§èƒ½ï¼Œæˆæœ¬å’Œå¤æ‚æ€§å‡ä½äºä½¿ç”¨ GPU å¹³å°ã€‚æ­¤å¤–ï¼Œç›¸æ¯”é»˜è®¤çš„ Scikit-Learnï¼Œè‹±ç‰¹å°”ä¼˜åŒ–ç‰ˆæœ¬çš„é€Ÿåº¦æå‡äº†é«˜è¾¾ 10 åˆ° 100 å€ï¼ˆSVC å’Œ kNN é¢„æµ‹ï¼‰$^2$ã€‚å‚èµ›äººå‘˜å¯ä»¥åˆ©ç”¨è‹±ç‰¹å°”å…è´¹ä¸ºæ‚¨æä¾›çš„ [oneAPI DevCloudäº‘æµ‹è¯•ç¯å¢ƒ](https://devcloud.intel.com/oneapi/) ä¸­çš„å„ç§ç¡¬ä»¶åŠè®¡ç®—èƒ½åŠ›ï¼Œæˆ–åˆ©ç”¨è‡ªå·±çš„ç¬”è®°æœ¬æˆ–å…¶ä»–é€‚åˆçš„ç¡¬ä»¶ç¯å¢ƒä½œä¸ºæ¯”èµ›ä»£ç çš„å¼€å‘ã€éªŒè¯å’Œæœ€ç»ˆä»£ç é€’äº¤çš„æµ‹è¯•ç¯å¢ƒã€‚ \n",
    "\n",
    "In this track of the Hackathon, we recommend to use a 3rd Generation or 4th Generation Intel&reg; Xeon&reg; based processor, software compoents will benefit from hardware features such as IntelÂ® AVX-512 instructions by deliver better preformance. 3rd Generation ntel Intel&reg; Xeon&reg; based processor - which is an Ice Lake CPU, is a popular hardware platform adopted by mainstream cloud servcie providers. It deliver competitive performance without the likely added cost and complexity of switching to a GPU platform and offer up to 10-100x faster Intel-optimized versions over default Scikit-Learn (SVC & kNN predict)$^2$. The participants can also utilize the free [oneAPI Developer Cloud](https://devcloud.intel.com/oneapi/) test environment and its hardware and computing capabilities, your own notebook or other appropriate development platform for code development, validataion and as your final test platform for code submission.\n",
    "\n",
    "é€šè¿‡ä¸‹é¢çš„å‘½ä»¤ï¼Œå¯ä»¥æŸ¥çœ‹ç¡¬ä»¶ç¯å¢ƒä¸­å¯¹åº”CPUçš„ç›¸åº”ç¡¬ä»¶èƒ½åŠ› (ä»…ä½œä¸ºç¤ºä¾‹)\n",
    "\n",
    "Through the below command, user will be able to view the hardware capability of the respective CPU ï¼ˆfor illustration purpose onlyï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8c8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ef17f-397e-4a0f-bc7b-9a851674e5b6",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"1_3\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">1.3 | å¯¼å…¥åº“å’Œæ•°æ®é›† | Import Libraries and Dataset</span></a>\n",
    "\n",
    "é¦–å…ˆï¼Œä½¿ç”¨<b>[è‹±ç‰¹å°”Â® Modin åˆ†å‘ç‰ˆ](https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-of-modin.html#gs.9hqdj4)</b>å¤„ç†ä¸æ¢ç´¢æ•°æ®ã€‚è‹±ç‰¹å°”Â® Modin åˆ†å‘ç‰ˆæ˜¯ä¸€ä¸ªåˆ†å¸ƒå¼ DataFrame åº“ï¼Œå¯æ ¹æ®æ•°æ®é›†çš„å¤§å°æ— ç¼æ‰©å±• pandas å·¥ä½œæµï¼Œæ”¯æŒä» 1 MB åˆ° 1 TB ä»¥ä¸Šçš„æ•°æ®é›†ã€‚é‡‡ç”¨æ ‡å‡† pandas ä¸€æ¬¡åªèƒ½ä½¿ç”¨ä¸€ä¸ªæ ¸å¿ƒã€‚ç„¶è€Œï¼Œå€ŸåŠ© Modin Dask* å¼•æ“ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ¸å¿ƒï¼Œä»è€Œæ›´å¿«é€Ÿåœ°å¤„ç†è¾ƒå¤§çš„æ•°æ®é›†ã€‚æ‚¨å¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼Œåˆ©ç”¨é‡‡ç”¨ Dask å¼•æ“çš„ Modinï¼š\n",
    "\n",
    "To get started, we'll be using the <b>[Intel&reg; Distribution of Modin](https://www.intel.com/content/www/us/en/developer/tools/oneapi/distribution-of-modin.html#gs.9hqdj4)</b> to process and explore the data. The Intel&reg; Distribution of Modin is a distributed DataFrame library designed to seamlessly scale your pandas workflow with the size of your dataset, supporting datasets that range from 1 MB to 1 TB+. With pandas, only one core is used at a time. However, with Modin's Dask* engine, all of the available cores are used, which allows you to work with very large datasets at much faster speeds. To utilize Modin with the Dask engine, you can use the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f915b5-3cef-4c68-9b8b-3bc896295e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "ray.init(_memory=16000 * 1024 * 1024, object_store_memory=500 * 1024 * 1024,_driver_object_store_memory=500 * 1024 * 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d037511-2a01-491f-9a69-b9fd6a109258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.pandas as pd\n",
    "from modin.config import Engine\n",
    "Engine.put(\"ray\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef60409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import daal4py as d4p\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "import warnings\n",
    "import pandas\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import plotly.io as pio\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pio.renderers.default='png' \n",
    "intel_pal, color=['#0071C5','#FCBB13'], ['#7AB5E1','#FCE7B2']\n",
    "temp=dict(layout=go.Layout(font=dict(family=\"Franklin Gothic\", size=12), \n",
    "                           height=500, width=1000))\n",
    "\n",
    "# Read data\n",
    "data = pandas.read_pickle('media/data_100000.pkl')\n",
    "print(\"Data shape: {}\\n\".format(data.shape))\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88aa08c",
   "metadata": {},
   "source": [
    "# <a class=\"anchor\" id=\"2\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #251cab, #3eb4f4, #251cab)\">2 | æ¢ç´¢æ€§æ•°æ®åˆ†æ | Exploratory Data Analysis</div></a>\n",
    "[è¿”å›è‡³ç›®å½• | Back to Table of contents](#TOC)\n",
    "\n",
    "åœ¨å·¥ä½œæµçš„ä¸‹ä¸€æ­¥ï¼Œæˆ‘ä»¬å°†è®¨è®ºï¼š | In the next step of the workflow, we'll explore:\n",
    "- æŸ¥çœ‹æ•°æ®é›†ä¸­çš„<b><span style=\"color:#211894;text-align:left\">ç¼ºå¤±å€¼</span></b> å’Œ<b><span style=\"color:#211894;text-align:left\">é‡å¤å€¼</span></b>ã€‚\n",
    "- æ•°æ®é›†çš„æè¿°æ€§ç»Ÿè®¡ï¼ŒåŒ…æ‹¬<b><span style=\"color:#211894;text-align:left\">å‡å€¼</span></b>ã€<b><span style=\"color:#211894;text-align:left\">æœ€å°å€¼</span></b>ã€<b><span style=\"color:#211894;text-align:left\">æœ€å¤§å€¼</span></b>ã€<b><span style=\"color:#211894;text-align:left\">æ ‡å‡†å·®</span></b>ã€<b><span style=\"color:#211894;text-align:left\">ååº¦</span></b>å’Œ<b><span style=\"color:#211894;text-align:left\">å³°åº¦</span></b>ã€‚\n",
    "- ç›®æ ‡å˜é‡ <b>`Asset_Label`</b> çš„åˆ†å¸ƒã€‚\n",
    "- æ•°å€¼å’Œç±»åˆ«ç‰¹å¾çš„<b><span style=\"color:#211894;text-align:left\">ä¸€å…ƒåˆ†å¸ƒ</span></b>å’Œ<b><span style=\"color:#211894;text-align:left\">äºŒå…ƒåˆ†å¸ƒ</span></b>ã€‚\n",
    "- æ•°æ®é›†ä¸­å˜é‡ä¹‹é—´çš„<b><span style=\"color:#211894;text-align:left\">ç›¸å…³æ€§</span></b>ã€‚\n",
    "<br>\n",
    "\n",
    "\n",
    "- Checking for <b><span style=\"color:#211894;text-align:left\">missing values</span></b> and <b><span style=\"color:#211894;text-align:left\">duplicates</span></b> in the dataset.\n",
    "- Descriptive statistics of the dataset, including the <b><span style=\"color:#211894;text-align:left\">mean</span></b>, <b><span style=\"color:#211894;text-align:left\">min</span></b>, <b><span style=\"color:#211894;text-align:left\">max</span></b>, <b><span style=\"color:#211894;text-align:left\">standard deviation</span></b>, <b><span style=\"color:#211894;text-align:left\">skewness</span></b>, and <b><span style=\"color:#211894;text-align:left\">kurtosis</span></b>.\n",
    "- The distribution of the target variable, <b>`Asset_Label`</b>.\n",
    "- <b><span style=\"color:#211894;text-align:left\">Univariate</span></b> and <b><span style=\"color:#211894;text-align:left\">bivariate</span></b> distributions of the numerical and categorical features.\n",
    "- <b><span style=\"color:#211894;text-align:left\">Correlations</span></b> between the variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c688f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.isna().sum())\n",
    "missing=data.isna().sum().sum()\n",
    "duplicates=data.duplicated().sum()\n",
    "print(\"\\nThere are {:,.0f} missing values in the data.\".format(missing))\n",
    "print(\"There are {:,.0f} duplicate records in the data.\".format(duplicates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40423271",
   "metadata": {},
   "source": [
    "æ•°æ®é›†çœ‹ä¸Šå»éå¸¸å¹²å‡€ï¼Œæ²¡æœ‰ç¼ºå¤±æˆ–é‡å¤è®°å½•ã€‚é‚£ä¹ˆï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ•°æ®é›†çš„æè¿°æ€§ç»Ÿè®¡ã€‚æè¿°æ€§ç»Ÿè®¡æä¾›äº†æ•°æ®åˆ†å¸ƒçš„é›†ä¸­è¶‹åŠ¿ã€ç¦»æ•£å’Œå½¢æ€ï¼Œä¸åŒ…æ‹¬ `NaN` å€¼ã€‚ | At first glance, the dataset appears to be fairly clean with no missing or duplicate records. Let's now look at the descriptive statistics of the dataset. Descriptive statistics provide a numerical summary of the central tendency, dispersion, and shape of the data's distribution, excluding `NaN` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6100c2-cd01-4e23-b559-61cebca517cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_stats(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to display descriptive statistics of numerical variables,\n",
    "    includes skewness & kurtosis.   \n",
    "    \"\"\"\n",
    "    \n",
    "    df=data.describe()\n",
    "    skewness=data.skew()\n",
    "    kurtosis=data.kurtosis()\n",
    "    df=df.append([skewness, kurtosis],ignore_index=True)\n",
    "    idx=pd.Series(['count','mean','std','min','25%','50%','75%','max','skewness','kurtosis'],name='Summary Statistic')\n",
    "    df=pd.concat([df,idx], axis=1).set_index('Summary Statistic')\n",
    "    display(df.style.format('{:,.3f}').\n",
    "        background_gradient(subset=(df.index[1:],df.columns[:]),\n",
    "                            cmap='GnBu'))\n",
    "\n",
    "display_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3defd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_target_dist(target_col):\n",
    "    \n",
    "    \"\"\"Function to display distribution of the target variable\"\"\"\n",
    "    \n",
    "    target=data[target_col].value_counts(normalize=True)\n",
    "    target.rename(index={1:'State 1',0:'State 0'},inplace=True)\n",
    "    fig=go.Figure()\n",
    "    fig.add_trace(go.Pie(labels=target.index, values=target*100, hole=.45, \n",
    "                         text=target.index, sort=False, showlegend=False,\n",
    "                         marker=dict(colors=color,line=dict(color=intel_pal,width=2.5)),\n",
    "                         hovertemplate = \"%{label}: <b>%{value:.2f}%</b><extra></extra>\"))\n",
    "    fig.update_layout(template=temp, title='Target Distribution',width=700,height=450,\n",
    "                      uniformtext_minsize=15, uniformtext_mode='hide')\n",
    "    fig.show() \n",
    "    \n",
    "plot_target_dist(target_col='Asset_Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1030c629-dc42-4f52-b62a-057cdcbb7865",
   "metadata": {},
   "source": [
    "å¦‚å›¾ä¸­çš„é»„è‰²éƒ¨åˆ†æ‰€ç¤ºï¼Œåœ¨æˆ‘ä»¬çš„æ•°æ®ä¸­ï¼Œçº¦ 40% çš„ç”µçº¿æ†è¢«è®¤å®šä¸ºéœ€è¦ç»´ä¿®ã€‚è€ƒè™‘åˆ°ç›®æ ‡å˜é‡çš„åˆ†å¸ƒæœ‰äº›å¤±è¡¡ï¼Œæˆ‘ä»¬å°†åœ¨äº¤å‰éªŒè¯ä¸­ä½¿ç”¨<b><span style=\"color:#211894;text-align:left\">åˆ†å±‚æŠ½æ ·</span></b>ã€‚ | About 40% of poles in our data have been identified as requiring maintenance, shown in the yellow portion of the chart. Given the imbalance in the distribution of our target variable, we will use <b><span style=\"color:#211894;text-align:left\">stratified sampling</span></b> during cross-validation.\n",
    "\n",
    "## <a class=\"anchor\" id=\"2_1\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #009ff2, #251cab)\">2.1 | æ•°å€¼å˜é‡ EDA | EDA of Numerical Variables</div></a>\n",
    "\n",
    "[è¿”å›è‡³ç›®å½• | Back to Table of Contents](#TOC)\n",
    "\n",
    "<br>\n",
    "\n",
    "<b><a class=\"anchor\" id=\"2_1_1\"><span style=\"padding:0px;color:#211894;margin:0;font-size:120%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">2.1.1 | ä¸€å…ƒåˆ†å¸ƒ | Univariate Distributions</span></a></b>\n",
    "\n",
    "åœ¨å¯¹ç›®æ ‡åˆ†å¸ƒæœ‰äº†æ›´å¥½çš„äº†è§£åï¼Œè®©æˆ‘ä»¬æ·±å…¥è§£è¯»æ•°æ®é›†ä¸­çš„ç‰¹å¾ï¼Œä»¥è¯†åˆ«ç‰¹å®šæ¨¡å¼ã€‚ä¸‹å›¾æ˜¯æ•°å€¼å˜é‡çš„æ ¸å¯†åº¦ä¼°è®¡ (KDE) å›¾ï¼Œä½¿ç”¨æ¦‚ç‡å¯†åº¦å‡½æ•°ç›´è§‚æ˜¾ç¤ºæ•°æ®å½¢æ€ã€‚\n",
    "\n",
    "Now that we have a better understanding of the target distribution, let's take a closer look at the features in the dataset to identify any patterns. The graphs below show the Kernel Density Estimation (KDE) plots of the numerical variables, which visually display the shape of the data using its probability density function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811931bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols,float_cols=[],['Asset_Label']\n",
    "for col in data.columns:\n",
    "    if data[col].value_counts().count()<10:\n",
    "        cat_cols.append(col)\n",
    "    else:\n",
    "        float_cols.append(col)\n",
    "        \n",
    "plot_df=data[float_cols]\n",
    "fig, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "fig.suptitle('Distribution of Numerical Variables',fontsize=16)\n",
    "row=0\n",
    "col=[0,1]*2\n",
    "for i, column in enumerate(plot_df.columns[1:]):\n",
    "    if (i!=0)&(i%2==0):\n",
    "        row+=1\n",
    "    sns.kdeplot(x=column, hue='Asset_Label', palette=intel_pal[::-1], hue_order=[1,0], \n",
    "                label=['State 1','State 0'], data=plot_df, \n",
    "                fill=True, linewidth=2.5, legend=False, ax=ax[row,col[i]])\n",
    "    ax[row,col[i]].tick_params(left=False, bottom=False)\n",
    "    ax[row,col[i]].set(title='\\n\\n{}'.format(column), xlabel='', ylabel=('Density' if i%2==0 else ''))\n",
    "\n",
    "handles, _ = ax[0,0].get_legend_handles_labels() \n",
    "fig.legend(labels=['State 1','State 0'], handles=reversed(handles), ncol=2, bbox_to_anchor=(0.18, 0.99))\n",
    "sns.despine(bottom=True, trim=True)\n",
    "plt.tight_layout(rect=[0, 0.2, 1, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afa0855-e6eb-4676-9a3d-64332513502a",
   "metadata": {},
   "source": [
    "ä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œå˜é‡ `Pole_Height` å…·æœ‰é’Ÿå½¢æ›²çº¿ï¼Œå‘ˆæ­£æ€åˆ†å¸ƒï¼Œè€Œå…¶ä»–å˜é‡æ›´æ¥è¿‘å‡åŒ€çš„éæ­£æ€åˆ†å¸ƒã€‚ | In the graphs above, we see the variable `Pole_Height` has a bell-shaped curve following a normal distribution, while the remaining variables more closely resemble uniform, non-normal distributions.\n",
    "\n",
    "<b><a class=\"anchor\" id=\"2_1_2\"><span style=\"padding:0px;color:#211894;margin:0;font-size:120%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">2.1.2 | äºŒå…ƒåˆ†å¸ƒ | Bivariate Distributions</span></a></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cee6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=make_subplots(rows=2,cols=2, subplot_titles=float_cols[1:])\n",
    "col=[1,2]*2\n",
    "row=0\n",
    "pal=sns.color_palette(\"GnBu\",20).as_hex()[9:][::3]\n",
    "for i, column in enumerate(data[float_cols].columns[1:]):\n",
    "    if i%2==0:\n",
    "        row+=1\n",
    "    df = pd.concat([data[column],data['Asset_Label']],axis=1)\n",
    "    df['bins'] = pd.cut(df[column],300)\n",
    "    df['mean'] = df.bins.apply(lambda x: x.mid)\n",
    "    df = df.groupby('mean')[column,'Asset_Label'].transform('mean')\n",
    "    df = df.drop_duplicates(subset=[column]).sort_values(by=column)\n",
    "    fig.add_trace(go.Scatter(x=df[column], y=df.Asset_Label, name=column,\n",
    "                             marker_color=pal[i],showlegend=False),\n",
    "                  row=row, col=col[i])\n",
    "    fig.update_xaxes(zeroline=False, row=row, col=col[i])\n",
    "    if i%2==0:\n",
    "        fig.update_yaxes(title='Target Probabilitiy',row=row,col=col[i])\n",
    "fig.update_layout(template=temp, title='Feature Relationships with Target', \n",
    "                  hovermode=\"x unified\",height=700,width=900)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced1771-2ef3-4398-8e7e-f9e2b7465ca2",
   "metadata": {},
   "source": [
    "ä¸ºäº†è¯†åˆ«ç›®æ ‡å˜é‡å’Œè¿ç»­ç‰¹å¾ä¹‹é—´çš„å…³ç³»ï¼Œä¸Šå›¾æè¿°äº†æ¯ä¸ªç‰¹å¾ä¸­æ‰€æœ‰å€¼çš„ç›®æ ‡æ¦‚ç‡ã€‚åœ¨æ–°æ—§ç¨‹åº¦ `Age` ä¸­ï¼Œæˆ‘ä»¬çœ‹åˆ°ä½¿ç”¨è¶…è¿‡ 45 å¹´å’Œä¸åˆ° 5 å¹´çš„ç”µçº¿æ†ç»´ä¿®æ¦‚ç‡æ›´é«˜ï¼Œè€Œåœ¨æµ·æ‹”é«˜åº¦  `Elevation` ä¸­ï¼Œæµ·æ‹”ä½äº 1,000 è‹±å°ºçš„ç»´ä¿®æ¦‚ç‡æ›´é«˜ã€‚ æ­¤å¤–ï¼Œæˆ‘ä»¬å‘ç°ç›®æ ‡å’Œç‰¹å¾ä¹‹é—´å­˜åœ¨éçº¿æ€§å…³ç³»ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬å¯èƒ½è¦å°è¯•éå‚æ•°æ¨¡å‹ã€‚ä¸‹æ–¹çš„æ•£ç‚¹å›¾çŸ©é˜µè¿›ä¸€æ­¥å°è¯äº†è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å‘ç°æ•°æ®çš„æ•°å€¼ç‰¹å¾ä¹‹é—´çº¿æ€§åº¦æå¼±ï¼Œç›¸å…³æ€§è¾ƒä½ã€‚ \n",
    "<br>\n",
    "\n",
    "To identify the relationships between the target variable and the continuous features, the graphs above depict the target probability across the values in each feature. In `Age`, we see the probability tends to be higher in poles that are above about 45 years and less than 5 years, while in `Elevation` this is true for poles that are below about 1,000 feet. In addition, we see there are non-linear relationships between the target and the features, which suggests we may want to try a nonparametric model. This is further demonstrated in the scatterplot matrix below, where we see very weak linearity and low correlations between the numerical features in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c7df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df=data[float_cols]\n",
    "fig, ax = plt.subplots(4,4, figsize=(16,18))\n",
    "fig.suptitle('Scatterplot Matrix of Numeric Variables\\nLog-Transformed',fontsize=16)\n",
    "for i, col in enumerate(float_cols[1:]):\n",
    "    for j, iter_col in enumerate(float_cols[1:]):\n",
    "        ax[i,j].hexbin(x=iter_col, y=col, data=plot_df, bins='log', gridsize=40, cmap='coolwarm')\n",
    "        ax[i,j].set(xlabel=iter_col, ylabel=(col if j%4==0 else ''))\n",
    "        ax[i,j].text(plot_df[iter_col].median(), plot_df[col].max(), \n",
    "                     'Correlation: {:.4f}'.format(plot_df[[col,iter_col]].corr().iloc[1,0]), \n",
    "                   ha=\"center\", va=\"center\",bbox=dict(boxstyle=\"round,pad=0.3\",fc=\"white\"))\n",
    "        ax[i,j].tick_params(left=False,bottom=False)    \n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.99])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f8ba2",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"2_2\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #009ff2, #251cab)\">2.2 | ç±»åˆ«å˜é‡ EDA | EDA of Categorical Variables</div></a>\n",
    "\n",
    "[è¿”å›è‡³ç›®å½• | Back to Table of Contents](#TOC)\n",
    "\n",
    "æˆ‘ä»¬åœ¨å‰é¢æ¢è®¨äº†æ•°å€¼å˜é‡ï¼Œæ¥ä¸‹æ¥çœ‹ä¸€ä¸‹ç±»åˆ«ç‰¹å¾çš„åˆ†å¸ƒã€‚æˆ‘ä»¬å·²ä½¿ç”¨ç‹¬çƒ­ç¼–ç å¯¹æ•°æ®é›†ä¸­çš„ç±»åˆ«ç‰¹å¾è¿›è¡Œäº†é¢„å¤„ç†ï¼Œç‹¬çƒ­ç¼–ç å®šæ€§åœ°è¡¨ç¤ºå˜é‡å­˜åœ¨ä¸å¦ï¼Œ1 ä»£è¡¨å­˜åœ¨ï¼Œ0 ä»£è¡¨ä¸å­˜åœ¨ã€‚ä¸‹å›¾æ˜¾ç¤ºäº†æ¯ä¸ªç‰¹å¾å‘ç”Ÿçš„é¢‘ç‡ï¼ŒæŒ‰ç…§ç›®æ ‡æ ‡ç­¾è¿›è¡Œç€è‰²ã€‚ | Now that we have explored our numerical variables, let's take a look at the distributions in the categorical features. The categorical features in the dataset have already been preprocessed using  one-hot encoding, which qualitatively represents the presence or absence of the variable with a corresponding 1 or 0, respectively. The graphs below show the frequency of the occurrence of each feature, colored by the target label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a50cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=10, cols=3, subplot_titles=[c for c in cat_cols if c!='Asset_Label'])\n",
    "rgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.6)) for i in intel_pal]\n",
    "row=0\n",
    "c=[1,2,3]*10\n",
    "for i, col in enumerate(c for c in cat_cols if c!='Asset_Label'):\n",
    "    if i%3==0:\n",
    "        row+=1\n",
    "    df=data.groupby(col)['Asset_Label'].value_counts().rename('count').reset_index()\n",
    "    fig.add_trace(go.Bar(x=df[df.Asset_Label==1][col], y=df[df.Asset_Label==1]['count'],width=.35,\n",
    "                         marker_color=rgb[1], marker_line=dict(color=intel_pal[1],width=2.5), \n",
    "                         hovertemplate='Value: %{x}<br>Count: %{y}',\n",
    "                         name='State 1', showlegend=(True if i==0 else False)),\n",
    "                  row=row, col=c[i])\n",
    "    fig.add_trace(go.Bar(x=df[df.Asset_Label==0][col], y=df[df.Asset_Label==0]['count'],width=.35,\n",
    "                         marker_color=rgb[0], marker_line=dict(color=intel_pal[0],width=2.5),\n",
    "                         hovertemplate='Value: %{x}<br>Count: %{y}',\n",
    "                         name='State 0', showlegend=(True if i==0 else False)),\n",
    "                  row=row, col=c[i])\n",
    "    if i%3==0:\n",
    "        fig.update_yaxes(title='Frequency',row=row,col=c[i])\n",
    "fig.update_layout(template=temp,title=\"Distributions of Categorical Variables\",\n",
    "                  legend=dict(orientation=\"h\",yanchor=\"bottom\",y=1.025,xanchor=\"right\",x=.2),\n",
    "                  barmode='group',height=2000,width=900)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b7d6c-0b72-4fa8-9511-c53b6472b19c",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"2_3\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #009ff2, #251cab)\">2.3 | ç›¸å…³æ€§ | Correlations</div></a>\n",
    "\n",
    "[è¿”å›è‡³ç›®å½• | Back to Table of Contents](#TOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415dae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_corr(corr):\n",
    "    \"\"\"\n",
    "    Function to plot bottom left triangle of correlation matrix\n",
    "    \"\"\"\n",
    "    mask=np.triu(np.ones_like(corr, dtype=bool))[1:,:-1]\n",
    "    corr=corr.iloc[1:,:-1].copy()\n",
    "    fig, ax = plt.subplots(figsize=(26,22))   \n",
    "    sns.heatmap(corr, mask=mask, vmin=-1, vmax=1, center=0, annot=True, fmt='.2f', \n",
    "                cmap='YlGnBu_r',lw=2, annot_kws={'fontsize':10,'fontweight':'bold'}, cbar=True)\n",
    "    ax.tick_params(left=False,bottom=False)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right',fontsize=12)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(),fontsize=12)\n",
    "    plt.title('Correlations between Utility Asset Maintenance Data\\n', fontsize=24)\n",
    "    plt.show()\n",
    "    \n",
    "def plot_target_corr(corr, target_col): \n",
    "    \"\"\"\n",
    "    Function to plot a bar chart of correlations between target and features, sorted in descending order\n",
    "    \"\"\"\n",
    "    corr=corr[target_col].sort_values(ascending=False)[1:]\n",
    "    pal=sns.color_palette(\"RdYlBu\",37).as_hex()\n",
    "    pal=[j for i,j in enumerate(pal) if i not in (17,18)]\n",
    "    rgb=['rgba'+str(matplotlib.colors.to_rgba(i,0.8)) for i in pal] \n",
    "    \n",
    "    fig=go.Figure()\n",
    "    fig.add_trace(go.Bar(x=corr.index, y=corr, marker_color=rgb,\n",
    "                         marker_line=dict(color=pal,width=2),\n",
    "                         hovertemplate='%{x} correlation with Target = %{y}',\n",
    "                         showlegend=False, name=''))\n",
    "    fig.update_layout(template=temp, title='Feature Correlations with Target (Asset Label)', \n",
    "                      yaxis_title='Correlation', margin=dict(b=160), xaxis_tickangle=45)\n",
    "    fig.show()\n",
    "    \n",
    "corr=data.corr()\n",
    "plot_corr(corr=corr)\n",
    "plot_target_corr(corr=corr, target_col='Asset_Label')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e4070",
   "metadata": {},
   "source": [
    "ä»ä¸Šæ–¹çš„æ•£ç‚¹å›¾çŸ©é˜µå¯ä»¥çœ‹å‡ºï¼Œæ•°å€¼ç‰¹å¾ä¹‹é—´çš„ç›¸å…³æ€§å¾ˆä½ï¼Œæ•°æ®é›†çš„å…¶ä»–ç‰¹å¾ä¹Ÿæ˜¯å¦‚æ­¤ã€‚åœ¨æˆ‘ä»¬çš„ç›®æ ‡å˜é‡ Asset_Label ä¸­ï¼ŒOriginal_Treatment_Untreatedæœ‰æœ€å¼ºçš„æ­£ç›¸å…³ï¼Œç›¸å…³æ€§ä¸º 0.311ï¼ŒDistrict_Wæœ‰æœ€å¼ºçš„è´Ÿç›¸å…³ï¼Œç›¸å…³æ€§ä¸º -0.172\n",
    "<br>\n",
    "As we saw in the scatterplot matrix above, there were very low correlations among the numerical features, which also remains true between the rest of the features in the dataset. Among our target variable, `Asset_Label`, the highest positive association exists with `Original_Treatment_Untreated` at 0.311 and the strongest negative relationship between `District_W` at -0.172\n",
    "\n",
    "# <a class=\"anchor\" id=\"3\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #3eb4f4, #251cab)\">3 | å»ºæ¨¡ | Modeling</div></a>\n",
    "\n",
    "[è¿”å›è‡³ç›®å½• | Back to Table of Contents](#TOC)\n",
    "\n",
    "åœ¨æœ¬èŠ‚ï¼Œæˆ‘ä»¬å°†æ¢ç´¢å¦‚ä½•ä½¿ç”¨ä¸åŒçš„æœºå™¨å­¦ä¹ æ¨¡å‹é¢„æµ‹ç”µçº¿æ†æ˜¯å¦éœ€è¦ç»´ä¿®ã€‚äºŒå…ƒåˆ†ç±»ä»»åŠ¡å¯é€‰æ‹©ä¸åŒçš„æ¨¡å‹ã€‚æœ€å¸¸ç”¨çš„æ¨¡å‹åŒ…æ‹¬é€»è¾‘å›å½’ã€æœ´ç´ è´å¶æ–¯ã€K æœ€è¿‘é‚»ã€æ”¯æŒå‘é‡æœºä»¥åŠé›†æˆæ–¹æ³•ï¼Œä¾‹å¦‚éšæœºæ£®æ—å’Œ XGBoostã€‚å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œç”±äºç‰¹å¾å’Œç›®æ ‡å˜é‡ä¹‹é—´æœ‰éçº¿æ€§å…³ç³»ï¼Œå› æ­¤æˆ‘ä»¬å°†æ¯”è¾ƒä¸¤ç§éå‚æ•°æ¨¡å‹çš„ä¼˜åŠ£ï¼šæ”¯æŒå‘é‡æœºå’Œ XGBoostã€‚\n",
    "<br>\n",
    "\n",
    "In this section, we will explore different Machine Learning models to predict whether or not a pole will need maintenance. With binary classification tasks, there are many different models to choose from. Some of the most common models include Logistic Regression, Naive Bayes, K-Nearest Neighbors, Support Vector Machines, and ensemble methods, like Random Forests and XGBoost. Since we saw there were nonlinear relationships between the features and the target variable in the graphs above, we will compare two nonparametric models: Support Vector Machines and XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5861ae46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(data, target_col, test_size):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to scale and split the data into training and test sets\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = RobustScaler()   \n",
    "    X = data.drop(target_col, axis=1)\n",
    "    y = data[target_col]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=21)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    print(\"Train Shape: {}\".format(X_train_scaled.shape))\n",
    "    print(\"Test Shape: {}\".format(X_test_scaled.shape))\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "def plot_model_res(model_name, y_test, y_prob):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to plot ROC/PR Curves and predicted target distribution\n",
    "    \"\"\"\n",
    "    \n",
    "    intel_pal=['#0071C5','#FCBB13']\n",
    "    color=['#7AB5E1','#FCE7B2']\n",
    "    \n",
    "    ## ROC & PR Curve ##\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    auprc = average_precision_score(y_test, y_prob)\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, \n",
    "                        shared_yaxes=True, \n",
    "                        subplot_titles=['Receiver Operating Characteristic<br>(ROC) Curve',\n",
    "                                        'Precision-Recall Curve<br>AUPRC = {:.3f}'.format(auprc)])\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=np.linspace(0,1,11), y=np.linspace(0,1,11), \n",
    "                             name='Baseline',mode='lines',legendgroup=1,\n",
    "                             line=dict(color=\"Black\", width=1, dash=\"dot\")), row=1,col=1)    \n",
    "    fig.add_trace(go.Scatter(x=fpr, y=tpr, line=dict(color=intel_pal[0], width=3), \n",
    "                             hovertemplate = 'True positive rate = %{y:.3f}, False positive rate = %{x:.3f}',\n",
    "                             name='AUC = {:.4f}'.format(roc_auc),legendgroup=1), row=1,col=1)\n",
    "    fig.add_trace(go.Scatter(x=recall, y=precision, line=dict(color=intel_pal[0], width=3), \n",
    "                             hovertemplate = 'Precision = %{y:.3f}, Recall = %{x:.3f}',\n",
    "                             name='AUPRC = {:.4f}'.format(auprc),showlegend=False), row=1,col=2)\n",
    "    fig.update_layout(template=temp, title=\"{} ROC and Precision-Recall Curves\".format(model_name), \n",
    "                      hovermode=\"x unified\", width=900,height=500,\n",
    "                      xaxis1_title='False Positive Rate (1 - Specificity)',\n",
    "                      yaxis1_title='True Positive Rate (Sensitivity)',\n",
    "                      xaxis2_title='Recall (Sensitivity)',yaxis2_title='Precision (PPV)',\n",
    "                      legend=dict(orientation='v', y=.07, x=.45, xanchor=\"right\",\n",
    "                                  bordercolor=\"black\", borderwidth=.5))\n",
    "    fig.show()\n",
    "    \n",
    "    ## Target Distribution ##     \n",
    "    plot_df=pd.DataFrame.from_dict({'State 0':(len(y_prob[y_prob<=0.5])/len(y_prob))*100, \n",
    "                                    'State 1':(len(y_prob[y_prob>0.5])/len(y_prob))*100}, \n",
    "                                   orient='index', columns=['pct'])\n",
    "    fig=go.Figure()\n",
    "    fig.add_trace(go.Pie(labels=plot_df.index, values=plot_df.pct, hole=.45, \n",
    "                         text=plot_df.index, sort=False, showlegend=False,\n",
    "                         marker=dict(colors=color,line=dict(color=intel_pal,width=2.5)),\n",
    "                         hovertemplate = \"%{label}: <b>%{value:.2f}%</b><extra></extra>\"))\n",
    "    fig.update_layout(template=temp, title='Predicted Target Distribution',width=700,height=450,\n",
    "                      uniformtext_minsize=15, uniformtext_mode='hide')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e715c96a",
   "metadata": {},
   "source": [
    "## <a class=\"anchor\" id=\"3_1\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">3.1 | é‡‡ç”¨è‹±ç‰¹å°”Â® Extension for Scikit-learn çš„æ”¯æŒå‘é‡åˆ†ç±»å™¨ | Support Vector Classifier with Intel&reg; Extension for Scikit-learn</span></a>\n",
    "\n",
    "[è¿”å›è‡³ç›®å½• | Back to Table of Contents](#TOC)\n",
    "\n",
    "æˆ‘ä»¬æ¼”ç¤ºçš„ç¬¬ä¸€ä¸ªæ¨¡å‹æ˜¯æ”¯æŒå‘é‡åˆ†ç±»å™¨ (SVC)ã€‚1992 å¹´ï¼ŒVapnik å’Œä»–çš„åŒäº‹æå‡ºäº†æ”¯æŒå‘é‡åˆ†ç±»å™¨ï¼ˆä½œä¸ºç®—æ³•ï¼‰ï¼Œä»¥æœ€å¤§åŒ–è®­ç»ƒæ ·æœ¬å’Œå†³ç­–è¾¹ç•Œä¹‹é—´çš„é—´éš” (margin)[[1](https://dl.acm.org/doi/10.1145/130385.130401)]ã€‚æˆ‘ä»¬å°†ä» Python Scikit-learn è½¯ä»¶åŒ…ä¸­å¯¼å…¥ SVC åº“ï¼Œå¹¶åˆ©ç”¨[è‹±ç‰¹å°”Â® Extension for Scikit-learn](https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html#gs.begztq)æä¾›çš„åŠ é€Ÿã€‚\n",
    "<br>\n",
    "\n",
    "The first model we will demonstrate is a Support Vector Classifier (SVC). Support Vector Classifiers were introduced in 1992 by Vapnik and colleagues as an algorithm that maximizes the margin between the training patterns and the decision boundary [[1](https://dl.acm.org/doi/10.1145/130385.130401)]. We will be importing the SVC library from the Scikit-learn package in Python and utilizing the accelerations provided with the [Intel&reg; Extension for Scikit-learn](https://www.intel.com/content/www/us/en/developer/tools/oneapi/scikit-learn.html#gs.begztq). \n",
    "\n",
    "è‹±ç‰¹å°”Â® Extension for Scikit-learn å¯è·¨å•èŠ‚ç‚¹å’Œå¤šèŠ‚ç‚¹é…ç½®ï¼Œæ— ç¼é›†æˆé¢å‘è‹±ç‰¹å°”Â® CPU å’Œ GPU çš„ scikit-learn åº”ç”¨ï¼ŒåŒæ—¶ç¼©çŸ­ç®—æ³•è¿è¡Œæ—¶é—´ã€‚å¦‚ä¸‹æ–¹çš„ä»£ç å•å…ƒæ‰€ç¤ºï¼Œåªéœ€è°ƒç”¨ patch_sklearn() å‡½æ•°ï¼Œå³å¯åˆ©ç”¨åŠ é€Ÿã€‚æ‚¨å¯ä»¥ç»§ç»­ä½¿ç”¨ç›¸åŒçš„ AI è½¯ä»¶åŒ…å’Œ scikit-learn åº“ï¼Œä¸ç”¨å†ä¿®æ”¹ä»£ç ã€‚æ‚¨å¯ä»¥åˆ©ç”¨è¡¥ä¸ï¼Œå°†æ”¯æŒçš„å¸¸è§ scikit-learn ç®—æ³•æ›¿æ¢ä¸ºå®ƒä»¬çš„ä¼˜åŒ–ç‰ˆæœ¬ã€‚è¯·è®¿é—®æˆ‘ä»¬çš„[å¼€å‘äººå‘˜æŒ‡å—](https://intel.github.io/scikit-learn-intelex/index.html)ï¼Œè¿›ä¸€æ­¥äº†è§£æ”¯æŒçš„ç®—æ³•ã€‚\n",
    "<br>\n",
    "\n",
    "The Intel&reg; Extension for Scikit-learn offers a seamless integration with scikit-learn applications for Intel&reg; CPUs and GPUs across single- and multi-node configurations while reducing algorithm run time. To take advantage of the accelerations, all you need to do is call the `patch_sklearn()` function as shown in the code cell below and continue using the same AI packages and scikit-learn libraries without any other changes to your code. The patch will replace supported stock scikit-learn algorithms with their optimized versions. To learn more about which algorithms are supported, please visit our [Developer Guide](https://intel.github.io/scikit-learn-intelex/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211463c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3317ac2d",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬å·²ç»å¯ç”¨äº†è‹±ç‰¹å°”Â® Extension for Scikit-learnï¼Œä¸‹ä¸€æ­¥æ˜¯æŠŠæ•°æ®åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•é›†ï¼Œå¹¶ä½¿ç”¨åˆ†å±‚ 3 é‡äº¤å‰éªŒè¯è°ƒä¼˜æ”¯æŒå‘é‡æœºçš„è¶…å‚æ•°ã€‚è¶…å‚æ•°è°ƒä¼˜çš„ç»“æœå’Œæµ‹è¯•é›†ä¸Šçš„æ¨¡å‹æ€§èƒ½å¦‚ä¸‹å›¾æ‰€ç¤ºã€‚ç”±äºè¿™æ˜¯ä¸€é¡¹äºŒå…ƒåˆ†ç±»ä»»åŠ¡ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å…³æ³¨çš„ä¸»è¦è¯„ä¼°æŒ‡æ ‡æ˜¯åˆ†æ•°å’Œ ROC æ›²çº¿ä¸‹é¢ç§¯ (AUC)ã€‚\n",
    "<br>\n",
    "Now that we've enabled the Intel&reg; Extension for Scikit-learn, we will split our data into training and test sets and tune the hyperparameters of the Support Vector Machine using stratified 3-fold cross-validation. The results of the hyperparameter tuning and model performance on the test set are shown in the graphs below. As this is a binary classification task, the main evaluation metrics we will be focusing on are the $F_{1}$ score and the Area Under the ROC Curve (AUC). \n",
    "\n",
    "<b>[$F_{1}$ æ•°å€¼](https://en.wikipedia.org/wiki/F-score)</b> æ˜¯é˜³æ€§ç±»ç²¾åº¦å’Œå¬å›ç‡ä¹‹é—´çš„è°ƒå’Œå¹³å‡æ•°ï¼Œç”¨ 1 æ¥è¡¨ç¤ºã€‚é€šè¿‡ä»¥ä¸‹è®¡ç®—æ–¹æ³•å¾—å‡ºï¼š\n",
    "\n",
    "\n",
    "The <b>[$F_{1}$ score](https://en.wikipedia.org/wiki/F-score)</b> represents the harmonic mean between the Precision and Recall of the positive class, indicated by a 1. It can be derived using the following calculation: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$F_1 = \\frac{2 * Precision * Recall}{Precision + Recall} = \\frac{2 * TP}{2 * TP + FP + FN}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "è™½ç„¶å¤§å¤šæ•°æŒ‡æ ‡æ˜¯ä½¿ç”¨ 0.5 åˆ†ç±»é˜ˆå€¼è®¡ç®—çš„ï¼Œä½†æ˜¯ <b>[ROC æ›²çº¿ä¸‹é¢ç§¯ (AUC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)</b>æ˜¯æ‰€æœ‰å¯èƒ½é˜ˆå€¼ä¸‹æ¨¡å‹æ€§èƒ½çš„æ€»ä½“è¡¨ç¤ºã€‚æ‚¨å¯ä»¥æŠŠå®ƒç†è§£ä¸ºä»é˜³æ€§ç±»ä¸­éšæœºé€‰æ‹©çš„è§‚æµ‹å€¼æ¯”ä»é˜´æ€§ç±»ä¸­éšæœºé€‰æ‹©çš„è§‚æµ‹å€¼æ›´é«˜çš„æ¦‚ç‡ã€‚\n",
    "\n",
    "While most metrics are calculated using a classification threshold of 0.5, the <b>[Area Under the ROC Curve (AUC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)</b> is an aggregate representation of the model's performance across all possible thresholds. It can be interpreted as the probability that a randomly-selected observation from the positive class will be ranked more highly than a randomly-selected observation from the negative class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ec64e-117d-4506-89b0-31aa2a18ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc, accuracy_score, f1_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb8511e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Train and Test datasets ##\n",
    "print(\"Preparing Train and Test datasets\")\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_data(data=data, \n",
    "                                                            target_col='Asset_Label', \n",
    "                                                            test_size=.25)\n",
    "## Initialize SVC model ##\n",
    "parameters = {\n",
    "    'class_weight': 'balanced',\n",
    "    'probability': True,\n",
    "    'random_state': 21}\n",
    "svc = SVC(**parameters)\n",
    "\n",
    "## Tune Hyperparameters ##\n",
    "strat_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=21)\n",
    "print(\"\\nTuning hyperparameters..\")\n",
    "grid = {\n",
    "    'C': np.logspace(-1, 1, 5),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "    }\n",
    "grid_search = RandomizedSearchCV(svc, param_distributions=grid, \n",
    "                                    cv=strat_kfold, n_iter=1, scoring='roc_auc', \n",
    "                                    verbose=1, n_jobs=-1, random_state=21)\n",
    "grid_search.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Done!\\nBest hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation AUC: {:.4f}\".format(grid_search.best_score_))\n",
    "\n",
    "svc = grid_search.best_estimator_\n",
    "svc_prob = svc.predict_proba(X_test)[:,1]\n",
    "svc_pred = pd.Series(svc.predict(X_test), name='Target')\n",
    "svc_auc = roc_auc_score(y_test, svc_prob)\n",
    "svc_f1 = f1_score(y_test, svc_pred)  \n",
    "   \n",
    "## Print model results ##\n",
    "print(\"\\nTest F1 accuracy: {:.2f}%, AUC: {:.5f}\".format(svc_f1*100,svc_auc))\n",
    "plot_model_res(model_name='SVC', y_test=y_test, y_prob=svc_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbf279",
   "metadata": {},
   "source": [
    "ä½¿ç”¨æ­£åˆ™åŒ–å‚æ•°çº¦ä¸º 3.16 çš„å¤šé¡¹å¼æ ¸ï¼Œæ”¯æŒå‘é‡åˆ†ç±»å™¨åœ¨æµ‹è¯•æ•°æ®é›†ä¸Šèƒ½å®ç°çš„[$F_{1}$ åˆ†æ•°](https://en.wikipedia.org/wiki/F-score)ä¸º 86.12%ï¼Œ[ROC æ›²çº¿ä¸‹é¢ç§¯ (AUC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)  ä¸º 0.91ã€‚\n",
    "åœ¨ä¸‹ä¸€èŠ‚ï¼Œæˆ‘ä»¬å°†å°è¯•ä½¿ç”¨å¦ä¸€ç§æœºå™¨å­¦ä¹ ç®—æ³• - XGBoost æ¨¡å‹æ¥æå‡ SVC çš„æ€§èƒ½ã€‚\n",
    "\n",
    "Using a polynomial kernel with a regularization parameter of about 3.16, the Support Vector Classifier achieved an [$F_{1}$ score](https://en.wikipedia.org/wiki/F-score) of 86.12% and an [Area Under the ROC Curve (AUC)](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc) of 0.91 on the test set.\n",
    "Next, we'll see if we we can improve on the performance of the SVC using a different type of machine learning algorithm, an XGBoost model.\n",
    "\n",
    "## <a class=\"anchor\" id=\"3_2\"><span style=\"padding:0px;color:#211894;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">3.2 | é’ˆå¯¹è‹±ç‰¹å°”Â® æ¶æ„ä¼˜åŒ–çš„ XGBoost | XGBoost Optimized for Intel&reg; Architectures</span></a>\n",
    "[è¿”å›è‡³ç›®å½• | Back to Table of Contents](#TOC)\n",
    "\n",
    "åœ¨æœ¬èŠ‚ï¼Œæˆ‘ä»¬å°†æ¢è®¨æç«¯æ¢¯åº¦æå‡ï¼ˆeXtreme Gradient Boostingï¼‰ç®—æ³•ï¼Œç®€ç§°ä¸º XGBoostã€‚2016 å¹´ï¼ŒChen å’Œ Guestrin å°† XGBoost ä½œä¸ºä¸€ç§å¯æ‰©å±•ç«¯åˆ°ç«¯å†³ç­–æ ‘æå‡ç³»ç»Ÿå‘å¸ƒå‡ºæ¥ã€‚è™½ç„¶å®ƒæ˜¯è¿‘å‡ å¹´æ‰å‡ºç°çš„ï¼Œä½†æ˜¯å·²æˆä¸ºæ•°æ®ç§‘å­¦å®¶å¹¿æ³›ä½¿ç”¨çš„ä¸€ä¸ªæ¨¡å‹ï¼Œåœ¨å¤šä¸ªæœºå™¨å­¦ä¹ æŒ‘æˆ˜èµ›ä¸Šå¤§æ”¾å¼‚å½©[[2](https://arxiv.org/abs/1603.02754)]ã€‚è‡ª XGBoost ç‰ˆæœ¬ 0.81 ä»¥æ¥ï¼Œè‹±ç‰¹å°”æ¨å‡ºäº†å¤šé¡¹ä¼˜åŒ–å¹¶å°†å…¶éƒ¨ç½²åˆ°ä¸Šæ¸¸çš„è½¯ä»¶åŒ…ï¼Œä»¥æœ€å¤§é™åº¦åœ°æå‡è®­ç»ƒæ€§èƒ½ã€‚\n",
    "\n",
    "In this section, we will explore the eXtreme Gradient Boosting algorithm, also known as XGBoost. XGBoost was introduced in 2016 by Chen and Guestrin as a scalable end-to-end decision tree boosting system. While it is still relatively new, it has become a widely used model by data scientists to achieve state-of-the-art results on many machine learning challenges [[2](https://arxiv.org/abs/1603.02754)]. Since XGBoost version 0.81, Intel&reg; has introduced many optimizations to maximize training performance that have been upstreamed into the package. \n",
    "\n",
    "æ­¥éª¤å’Œä¸Šæ–‡çš„ç›¸åŒï¼Œé¦–å…ˆå°†æ•°æ®åˆ†ä¸ºè®­ç»ƒå’Œæµ‹è¯•é›†ï¼Œç„¶åä½¿ç”¨åˆ†å±‚ 3 é‡äº¤å‰éªŒè¯è°ƒä¼˜ XGBoost æ¨¡å‹çš„è¶…å‚æ•°ã€‚æˆ‘ä»¬ä¸€æ—¦æ‰¾åˆ°äº† XGBoost æ¨¡å‹çš„æœ€ä½³è¶…å‚æ•°ï¼Œä¾¿å°†å…¶è½¬æ¢ä¸º daal4py æ¨¡å‹ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥æ”¹è¿›é¢„æµ‹æ—¶é—´æ€§èƒ½ã€‚[Daal4py](https://intelpython.github.io/daal4py/) å¯ä» [è‹±ç‰¹å°”Â® oneAPI æ•°æ®åˆ†æåº“ (oneDAL)](https://oneapi-src.github.io/oneDAL/)  ä¸­ä¸‹è½½ï¼Œå®ƒå°†åˆ©ç”¨åº•å±‚çš„è‹±ç‰¹å°”Â® é«˜çº§çŸ¢é‡æ‰©å±•æŒ‡ä»¤é›†ï¼ˆè‹±ç‰¹å°”Â® AVX-512ï¼‰ç¡¬ä»¶ï¼Œæœ€å¤§é™åº¦åœ°æé«˜è‹±ç‰¹å°”Â® è‡³å¼ºÂ® å¤„ç†å™¨ä¸Šçš„æ¢¯åº¦æå‡æ€§èƒ½ã€‚\n",
    "\n",
    "We will follow the same steps as above, beginning with splitting our data into training and test sets and tuning the hyperparameters of the XGBoost model using stratified 3-fold cross-validation. Once we have found the best hyperparameters for the XGBoost model, we will convert it to a daal4py model for further improved performance on prediction time. [Daal4py](https://intelpython.github.io/daal4py/) can be downloaded from the [Intel&reg; oneAPI Data Analytics Library (oneDAL)](https://oneapi-src.github.io/oneDAL/) and utilizes the underlying Intel&reg; Advanced Vector Extensions (Intel&reg; AVX-512) hardware to maximize gradient boosting performance on Intel&reg; Xeon&reg; processors.\n",
    "\n",
    "ä½¿ç”¨ `get_gbt_model_from_xgboost()` å‡½æ•°ç¼–å†™ä¸€è¡Œä»£ç ï¼Œå³å¯å°†è°ƒä¼˜åçš„ XGBoost æ¨¡å‹è½»æ¾è½¬æ¢ä¸º Daal4pyã€‚ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ Daal4py çš„é¢„æµ‹å‡½æ•°å‘é€ç»è¿‡è®­ç»ƒçš„æ¨¡å‹å’Œè¾“å…¥æ•°æ®ï¼Œä»¥è®¡ç®—æµ‹è¯•é›†ä¸Šçš„æ¦‚ç‡ã€‚\n",
    "\n",
    "To convert a tuned XGBoost model to Daal4py is very easy to do in one line of code with the `get_gbt_model_from_xgboost()` function. Then, you can send the trained model along with the input data using Daal4py's prediction function to calculate the probabilities on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe326e9e-67f5-47b4-b955-41a62707c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare Train and Test datasets ##\n",
    "print(\"Preparing Train and Test datasets\")\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_data(data=data,\n",
    "                                                            target_col='Asset_Label', \n",
    "                                                            test_size=.25)\n",
    "    \n",
    "## Initialize XGBoost model ##\n",
    "ratio = float(np.sum(y_train == 0)) / np.sum(y_train == 1)\n",
    "parameters = {'scale_pos_weight': ratio.round(2), \n",
    "                'tree_method': 'hist',\n",
    "                'random_state': 21}\n",
    "xgb_model = XGBClassifier(**parameters)\n",
    "\n",
    "## Tune hyperparameters ##\n",
    "strat_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=21)\n",
    "print(\"\\nTuning hyperparameters..\")\n",
    "grid = {'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'max_depth': [3, 4, 5],\n",
    "        }\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid=grid, \n",
    "                            cv=strat_kfold, scoring='roc_auc', \n",
    "                            verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Done!\\nBest hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Best cross-validation AUC: {:.4f}\".format(grid_search.best_score_))\n",
    "    \n",
    "## Convert XGB model to daal4py ##\n",
    "xgb = grid_search.best_estimator_\n",
    "daal_model = d4p.get_gbt_model_from_xgboost(xgb.get_booster())\n",
    "\n",
    "## Calculate predictions ##\n",
    "daal_prob = d4p.gbt_classification_prediction(nClasses=2,\n",
    "    resultsToEvaluate=\"computeClassLabels|computeClassProbabilities\",\n",
    "    fptype='float').compute(X_test, daal_model).probabilities # or .predictions\n",
    "xgb_pred = pd.Series(np.where(daal_prob[:,1]>.5, 1, 0), name='Target')\n",
    "xgb_auc = roc_auc_score(y_test, daal_prob[:,1])\n",
    "xgb_f1 = f1_score(y_test, xgb_pred)  \n",
    "    \n",
    "## Plot model results ##\n",
    "print(\"\\nTest F1 Accuracy: {:.2f}%, AUC: {:.5f}\".format(xgb_f1*100, xgb_auc)) \n",
    "plot_model_res(model_name='XGBoost', y_test=y_test, y_prob=daal_prob[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfc3bbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "XGBoost æ¨¡å‹å°† F1 å‡†ç¡®ç‡å’Œ AUC åˆ†åˆ«æé«˜è‡³ 90% ä»¥ä¸Šå’Œ 0.94ã€‚ | The XGBoost model improved both the F1 accuracy and the AUC to over 90% and 0.94, respectively.\n",
    "\n",
    "# <a class=\"anchor\" id=\"4\"><div style=\"padding:20px;color:white;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:#251cab;overflow:hidden;background:linear-gradient(90deg, navy, #3eb4f4, #251cab)\">4 | ç»“è®º | Conclusion</div></a>\n",
    "[è¿”å›è‡³ç›®å½• | Back to Table of Contents](#TOC)\n",
    "\n",
    "è°¢è°¢æ‚¨çš„é˜…è¯»ï¼æœ¬å‚è€ƒå¥—ä»¶å®ç°ä¸ºå…¬ç”¨äº‹ä¸šå®¢æˆ·æä¾›äº†èµ„äº§ç»´æŠ¤é¢„æµ‹çš„æ€§èƒ½ä¼˜åŒ–æŒ‡å—ï¼Œå¯è½»æ¾æ‰©å±•è‡³ç±»ä¼¼çš„è¡Œä¸šå’Œä½¿ç”¨åœºæ™¯ã€‚å¦‚æœæ‚¨æƒ³äº†è§£æ›´å¤šä¿¡æ¯å¹¶ä¸”ä¸‹è½½è‹±ç‰¹å°”Â® é¢„æµ‹æ€§èµ„äº§ç»´æŠ¤ AI å‚è€ƒå¥—ä»¶ï¼Œè¯·è®¿é—®æ­¤é“¾æ¥ï¼šhttps://github.com/oneapi-src/predictive-health-analytics. æ‚¨ä¹Ÿå¯ä»¥ç‚¹å‡»[æ­¤å¤„](https://www.intel.com/content/www/us/en/developer/videos/optimize-utility-maintenance-prediction-ai-kit.html)ï¼Œè§‚çœ‹æœ¬ notebook çš„è§†é¢‘æ¼”ç¤ºã€‚\n",
    "\n",
    "Thank you for reading! This reference kit implementation provides a performance-optimized guide around the prediction of Asset Maintenance for Utility customers that can easily be scaled across similar industries and use cases. If you would like to learn more and download the Intel&reg; Predictive Asset Maintenance AI Reference Kit, please visit this link: https://github.com/oneapi-src/predictive-health-analytics. You may also watch a video demo of this notebook [here](https://www.intel.com/content/www/us/en/developer/videos/optimize-utility-maintenance-prediction-ai-kit.html).\n",
    "\n",
    "## <a class=\"anchor\" id=\"5\"><span style=\"padding:0px;color:#251cab;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">å‚è€ƒèµ„æ–™ | References</span></a>\n",
    "\n",
    "1. Boser, B., Guyon, I., Vapnik, V. (1992, July). A training algorithm for optimal margin classifiers. In: <i>Proceedings of the Fifth Annual Workshop on Computational Learning Theory</i>.\n",
    "2. Chen, T., & Guestrin, C. (2016, August). Xgboost: A scalable tree boosting system. In: <i>Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining </i>(pp. 785-794).\n",
    "\n",
    "## <span style=\"padding:0px;color:#251cab;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">è„šæ³¨ | Footnotes</span>\n",
    "$^1$ ç”µçº¿æ†ï¼šç»´æŠ¤æˆ–æ›´æ¢ã€‚ç¾å›½å…¬ç”¨äº‹ä¸šåˆä½œä¼™ä¼´ã€‚2020 å¹´ 8 æœˆ 3 æ—¥ã€‚ | Utility Poles: Maintenance or Replacement.â€¯Utility Partners of America. August 3, 2020.  \n",
    "$^2$ è¯·å‚è§ [www.intel.com/3gen-xeon-config](www.intel.com/3gen-xeon-config) çš„ç¬¬ [117] æ¡ã€‚ç»“æœå¯èƒ½ä¸åŒã€‚ | See [117] at [www.intel.com/3gen-xeon-config](www.intel.com/3gen-xeon-config). Results may vary.\n",
    "\n",
    "\n",
    "## <span style=\"padding:0px;color:#251cab;margin:0;font-size:100%;text-align:left;display:fill;border-radius:10px;background-color:white;overflow:hidden\">Notices & Disclaimers</span>\n",
    "Intel optimizations, for Intel compilers or other products, may not optimize to the same degree for non-Intel products.  \n",
    "Performance varies by use, configuration and other factors. Learn more on the Performance Index site.   \n",
    "Performance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates.  See backup for configuration details.  No product or component can be absolutely secure.   \n",
    "Your costs and results may vary.   \n",
    "Intel technologies may require enabled hardware, software or service activation.  \n",
    "&copy; Intel Corporation.  Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries.  Other names and brands may be claimed as the property of others.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack-ml-ray",
   "language": "python",
   "name": "hack-ml-ray"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "864135a569a0074a0e5fb4f63ce6676f9823c9f9a01192383dbd7b79504f9de8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
